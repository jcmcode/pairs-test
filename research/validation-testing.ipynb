{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817e382e",
   "metadata": {},
   "source": [
    "# Validation Testing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494477d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_ta_classic as ta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import OPTICS\n",
    "from statsmodels.tsa.stattools import coint, grangercausalitytests\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "import statsmodels.api as sm\n",
    "import pykalman as KalmanFilter\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d3183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "stocks = [\n",
    "    # S&P for Beta\n",
    "    \"^GSPC\",\n",
    "    # Megacap Leaders & Generalists\n",
    "    \"NVDA\", \"TSM\", \"AVGO\", \"AMD\", \"INTC\", \"MU\", \"TXN\", \"QCOM\", \"ADI\", \"MCHP\",\n",
    "    \n",
    "    # Equipment & Manufacturing\n",
    "    \"ASML\", \"AMAT\", \"LRCX\", \"KLAC\", \"TER\", \"ENTG\", \"NVMI\", \"TOELY\",\n",
    "    \n",
    "    # Specialized\n",
    "    \"ON\", \"NXPI\", \"STM\", \"LSCC\", \"MPWR\", \"QRVO\", \"SWKS\", \"ALAB\", \"CRDO\",\n",
    "    \n",
    "    # Intellectual Property & Design Software\n",
    "    \"ARM\", \"SNPS\", \"CDNS\", \"CEVA\",\n",
    "    \n",
    "    # Memory & Storage\n",
    "    \"WDC\", \"STX\", # Removed extra \"MU\" here\n",
    "    \n",
    "    # Emerging & Mid-Cap\n",
    "    \"GFS\", \"MRVL\", \"MTSI\", \"POWI\", \"SMTC\", \"VICR\", \"CAMT\"\n",
    "]\n",
    "\n",
    "def fetch_data(stocks):\n",
    "    data = yf.download(tickers=stocks, period=\"252d\", interval=\"1h\", group_by='ticker', auto_adjust=True, threads=True)\n",
    "    \n",
    "    price_series_list = []\n",
    "    for s in stocks:\n",
    "        try: \n",
    "            if s in data:\n",
    "                series = data[s]['Close']\n",
    "                series.name = s\n",
    "                price_series_list.append(series)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if price_series_list:\n",
    "        df = pd.concat(price_series_list, axis=1)\n",
    "        df = df.ffill() \n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df = fetch_data(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING FOR TRANSIENT REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Clean and Prepare Price Data\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    if 'Close' in df.columns.get_level_values(0):\n",
    "        df = df['Close']\n",
    "    elif 'Close' in df.columns.get_level_values(1):\n",
    "        df = df.xs('Close', axis=1, level=1)\n",
    "\n",
    "# 2. Base Calculations\n",
    "returns_df = df.pct_change().dropna()\n",
    "market_returns = returns_df['^GSPC']\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL CHANGE: Multi-Timeframe Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "# SHORT-TERM WINDOW (Transient regime detection)\n",
    "window_short = 50  # ~1 week of hourly data - ALIGNED WITH TRADE DURATION\n",
    "\n",
    "# MEDIUM-TERM WINDOW (Context/stability check)\n",
    "window_medium = 147  # ~3 weeks - your original window\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Short-term window: {window_short} hours (~1 week)\")\n",
    "print(f\"Medium-term window: {window_medium} hours (~3 weeks)\")\n",
    "print(f\"Optimizing for transient events: 10-50 hour duration\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3A. SHORT-TERM FEATURES (Primary clustering features)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating SHORT-TERM features (primary regime indicators)...\")\n",
    "\n",
    "# Feature A: SHORT-TERM Volatility (Recent risk behavior)\n",
    "rolling_vol_short = returns_df.rolling(window=window_short).std() * np.sqrt(252 * 7)\n",
    "\n",
    "# Feature B: SHORT-TERM Beta to SPX (Recent market sensitivity)\n",
    "rolling_cov_mkt_short = returns_df.rolling(window=window_short).cov(market_returns)\n",
    "rolling_mkt_var_short = market_returns.rolling(window=window_short).var()\n",
    "rolling_beta_spx_short = rolling_cov_mkt_short.divide(rolling_mkt_var_short, axis=0)\n",
    "\n",
    "# Feature C: SHORT-TERM Beta to Sector (Recent sector coupling)\n",
    "sector_returns = returns_df.drop(columns=['^GSPC'], errors='ignore').mean(axis=1)\n",
    "rolling_cov_sector_short = returns_df.rolling(window=window_short).cov(sector_returns)\n",
    "rolling_sector_var_short = sector_returns.rolling(window=window_short).var()\n",
    "rolling_beta_sector_short = rolling_cov_sector_short.divide(rolling_sector_var_short, axis=0)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3B. MEDIUM-TERM FEATURES (Context/stability indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating MEDIUM-TERM features (context indicators)...\")\n",
    "\n",
    "# These help identify if current behavior is unusual vs. longer-term baseline\n",
    "rolling_vol_medium = returns_df.rolling(window=window_medium).std() * np.sqrt(252 * 7)\n",
    "\n",
    "rolling_cov_mkt_medium = returns_df.rolling(window=window_medium).cov(market_returns)\n",
    "rolling_mkt_var_medium = market_returns.rolling(window=window_medium).var()\n",
    "rolling_beta_spx_medium = rolling_cov_mkt_medium.divide(rolling_mkt_var_medium, axis=0)\n",
    "\n",
    "rolling_cov_sector_medium = returns_df.rolling(window=window_medium).cov(sector_returns)\n",
    "rolling_sector_var_medium = sector_returns.rolling(window=window_medium).var()\n",
    "rolling_beta_sector_medium = rolling_cov_sector_medium.divide(rolling_sector_var_medium, axis=0)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3C. INSTANTANEOUS FEATURES (Momentum/Overbought indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating INSTANTANEOUS features (momentum indicators)...\")\n",
    "\n",
    "# Feature D: RSI (Momentum/Overextended) - Keep at 14 (standard)\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi_df = df.apply(calculate_rsi)\n",
    "\n",
    "# Feature E: Short Term Momentum (5-period return)\n",
    "momentum_5h = df.pct_change(periods=5)\n",
    "\n",
    "# Feature F: Momentum Acceleration (change in momentum)\n",
    "momentum_10h = df.pct_change(periods=10)\n",
    "momentum_acceleration = momentum_5h - momentum_10h\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3D. REGIME CHANGE INDICATORS (New!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating REGIME CHANGE indicators...\")\n",
    "\n",
    "# Detect when short-term behavior diverges from medium-term baseline\n",
    "# This helps identify when a NEW regime is forming\n",
    "\n",
    "# Volatility Regime Shift (is vol spiking vs. baseline?)\n",
    "vol_regime_shift = (rolling_vol_short - rolling_vol_medium) / rolling_vol_medium\n",
    "\n",
    "# Beta Regime Shift (is market sensitivity changing?)\n",
    "beta_spx_regime_shift = rolling_beta_spx_short - rolling_beta_spx_medium\n",
    "beta_sector_regime_shift = rolling_beta_sector_short - rolling_beta_sector_medium\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Assemble the Master Time-Series DataFrame (ts_df)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nAssembling time-series dataframe...\")\n",
    "\n",
    "ts_data_list = []\n",
    "\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC' or ticker not in df.columns: \n",
    "        continue\n",
    "    \n",
    "    # Extract features for this specific ticker\n",
    "    temp_df = pd.DataFrame({\n",
    "        # Price & Returns (baseline)\n",
    "        'Price': df[ticker],\n",
    "        'Returns': returns_df[ticker],\n",
    "        \n",
    "        # SHORT-TERM FEATURES (Primary clustering features)\n",
    "        'Vol_Short': rolling_vol_short[ticker],\n",
    "        'Beta_SPX_Short': rolling_beta_spx_short[ticker],\n",
    "        'Beta_Sector_Short': rolling_beta_sector_short[ticker],\n",
    "        \n",
    "        # MEDIUM-TERM FEATURES (Context)\n",
    "        'Vol_Medium': rolling_vol_medium[ticker],\n",
    "        'Beta_SPX_Medium': rolling_beta_spx_medium[ticker],\n",
    "        'Beta_Sector_Medium': rolling_beta_sector_medium[ticker],\n",
    "        \n",
    "        # INSTANTANEOUS FEATURES\n",
    "        'RSI': rsi_df[ticker],\n",
    "        'Momentum_5H': momentum_5h[ticker],\n",
    "        'Momentum_10H': momentum_10h[ticker],\n",
    "        'Momentum_Accel': momentum_acceleration[ticker],\n",
    "        \n",
    "        # REGIME CHANGE INDICATORS (New!)\n",
    "        'Vol_Regime_Shift': vol_regime_shift[ticker],\n",
    "        'Beta_SPX_Regime_Shift': beta_spx_regime_shift[ticker],\n",
    "        'Beta_Sector_Regime_Shift': beta_sector_regime_shift[ticker],\n",
    "        \n",
    "    }, index=df.index)\n",
    "    \n",
    "    temp_df['Ticker'] = ticker\n",
    "    ts_data_list.append(temp_df)\n",
    "\n",
    "if ts_data_list:\n",
    "    ts_df = pd.concat(ts_data_list).reset_index().set_index(['Datetime', 'Ticker'])\n",
    "    \n",
    "    # Drop NaNs created by rolling windows\n",
    "    initial_rows = len(ts_df)\n",
    "    ts_df = ts_df.dropna()\n",
    "    dropped_rows = initial_rows - len(ts_df)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TIME-SERIES DATAFRAME CREATED SUCCESSFULLY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(ts_df):,}\")\n",
    "    print(f\"Rows dropped (NaN): {dropped_rows:,} ({dropped_rows/initial_rows:.1%})\")\n",
    "    print(f\"Date range: {ts_df.index.get_level_values('Datetime').min()} to {ts_df.index.get_level_values('Datetime').max()}\")\n",
    "    print(f\"Unique tickers: {ts_df.index.get_level_values('Ticker').nunique()}\")\n",
    "    print(f\"\\nFeature columns: {len([c for c in ts_df.columns if c not in ['Price', 'Returns', 'Ticker']])}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(ts_df.head())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. OPTIONAL: Static Fundamental DataFrame (Keep or Remove?)\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: For transient regime detection, fundamentals are less relevant\n",
    "# Transient coupling is driven by events/news, not fundamental similarity\n",
    "# Consider REMOVING this section unless you plan to use it for filtering\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SKIPPING STATIC FUNDAMENTALS (Not relevant for transient detection)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"Transient coupling is driven by events/market dynamics, not fundamental profiles.\")\n",
    "print(\"If you want to filter pairs by fundamentals later, re-enable this section.\\n\")\n",
    "\n",
    "# Uncomment below if you want to keep fundamentals\n",
    "\"\"\"\n",
    "fundamental_list = []\n",
    "print(\"Fetching Static Fundamentals...\")\n",
    "\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC': continue\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        info = t.info\n",
    "        \n",
    "        fundamental_list.append({\n",
    "            'Ticker': ticker,\n",
    "            'Sector': info.get('sector', 'Unknown'),\n",
    "            'Industry': info.get('industry', 'Unknown'),\n",
    "            'Market_Cap': info.get('marketCap', np.nan),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "        continue\n",
    "\n",
    "static_df = pd.DataFrame(fundamental_list).set_index('Ticker')\n",
    "print(\"Static DataFrame (static_df) Created Successfully!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING COMPLETE - Ready for clustering\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e45dcd",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTICS CLUSTERING FOR TRANSIENT REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "if 'ts_df' not in locals():\n",
    "    raise ValueError(\"Please run the Feature Engineering cell to create 'ts_df' first.\")\n",
    "\n",
    "# Clean duplicates\n",
    "ts_df = ts_df[~ts_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Check Density\n",
    "density = ts_df.groupby(level='Datetime').size()\n",
    "valid_timestamps = density[density >= 5].index\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRANSIENT REGIME DETECTION - OPTICS CLUSTERING\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Data Density: {len(valid_timestamps)} valid hourly timestamps\")\n",
    "print(f\"Date Range: {valid_timestamps.min()} to {valid_timestamps.max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# USE EXISTING FEATURES (Your current column names)\n",
    "# ============================================================================\n",
    "features_to_cluster = ['Returns', 'Rolling_Vol', 'Beta_SPX', 'Beta_Sector', 'RSI', 'Momentum_5H']\n",
    "print(f\"\\nUsing features: {features_to_cluster}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLUSTERING LOOP - Hourly Regime Detection\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1: Running OPTICS Clustering (Hourly Snapshots)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_results = []\n",
    "cluster_quality_log = []\n",
    "\n",
    "for i, ts in enumerate(valid_timestamps):\n",
    "    try:\n",
    "        snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "        if len(snapshot) < 5: \n",
    "            continue\n",
    "        \n",
    "        # Scale & PCA (Dimensionality Reduction)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(snapshot)\n",
    "        \n",
    "        pca = PCA(n_components=0.90)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # OPTICS Clustering\n",
    "        optics = OPTICS(min_samples=3, metric='euclidean', xi=0.05, min_cluster_size=3)\n",
    "        optics.fit(X_pca)\n",
    "        \n",
    "        # ===== CLUSTER QUALITY VALIDATION (RELAXED) =====\n",
    "        unique_clusters = len(set(optics.labels_)) - (1 if -1 in optics.labels_ else 0)\n",
    "        noise_count = (optics.labels_ == -1).sum()\n",
    "        noise_pct = noise_count / len(optics.labels_)\n",
    "        total_stocks = len(optics.labels_)\n",
    "        \n",
    "        # Quality Metrics\n",
    "        quality_metrics = {\n",
    "            'Datetime': ts,\n",
    "            'Total_Stocks': total_stocks,\n",
    "            'Unique_Clusters': unique_clusters,\n",
    "            'Noise_Count': noise_count,\n",
    "            'Noise_Pct': noise_pct,\n",
    "            'PCA_Components': X_pca.shape[1],\n",
    "            'Variance_Explained': pca.explained_variance_ratio_.sum()\n",
    "        }\n",
    "        \n",
    "        # RELAXED Quality Filters\n",
    "        is_valid = True\n",
    "        skip_reason = None\n",
    "        \n",
    "        if unique_clusters < 1:\n",
    "            is_valid = False\n",
    "            skip_reason = \"No clusters found (all noise)\"\n",
    "        elif noise_pct > 0.75:  # Relaxed to 75%\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Too noisy ({noise_pct:.1%} noise)\"\n",
    "        elif unique_clusters > total_stocks * 0.75:\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Over-fragmented ({unique_clusters} clusters for {total_stocks} stocks)\"\n",
    "        \n",
    "        quality_metrics['Is_Valid'] = is_valid\n",
    "        quality_metrics['Skip_Reason'] = skip_reason\n",
    "        cluster_quality_log.append(quality_metrics)\n",
    "        \n",
    "        # Store valid clusters\n",
    "        if is_valid:\n",
    "            snapshot['Cluster_ID'] = optics.labels_\n",
    "            snapshot['Datetime'] = ts\n",
    "            snapshot['Num_Clusters'] = unique_clusters\n",
    "            snapshot['Noise_Pct'] = noise_pct\n",
    "            cluster_results.append(snapshot.reset_index())\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 100 == 0:\n",
    "            valid_so_far = len(cluster_results)\n",
    "            print(f\"  Processed {i+1}/{len(valid_timestamps)} timestamps... ({valid_so_far} valid so far)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        cluster_quality_log.append({\n",
    "            'Datetime': ts,\n",
    "            'Is_Valid': False,\n",
    "            'Skip_Reason': f\"Error: {str(e)[:50]}\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "if not cluster_results:\n",
    "    raise ValueError(\"No valid clusters found. Check your data quality and OPTICS parameters.\")\n",
    "\n",
    "cluster_history = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1 COMPLETE: Cluster Quality Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLUSTER QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "df_quality = pd.DataFrame(cluster_quality_log)\n",
    "\n",
    "total_timestamps = len(df_quality)\n",
    "valid_timestamps_count = df_quality['Is_Valid'].sum()\n",
    "invalid_timestamps_count = total_timestamps - valid_timestamps_count\n",
    "\n",
    "print(f\"\\nTimestamp Analysis:\")\n",
    "print(f\"  Total timestamps processed: {total_timestamps}\")\n",
    "print(f\"  Valid clustering windows: {valid_timestamps_count} ({valid_timestamps_count/total_timestamps:.1%})\")\n",
    "print(f\"  Invalid/skipped windows: {invalid_timestamps_count} ({invalid_timestamps_count/total_timestamps:.1%})\")\n",
    "\n",
    "if invalid_timestamps_count > 0:\n",
    "    print(f\"\\nSkip Reasons:\")\n",
    "    skip_summary = df_quality[~df_quality['Is_Valid']]['Skip_Reason'].value_counts()\n",
    "    for reason, count in skip_summary.items():\n",
    "        print(f\"  - {reason}: {count} ({count/invalid_timestamps_count:.1%})\")\n",
    "\n",
    "# Valid cluster statistics\n",
    "valid_quality = df_quality[df_quality['Is_Valid']]\n",
    "if len(valid_quality) > 0:\n",
    "    print(f\"\\nValid Cluster Statistics:\")\n",
    "    print(f\"  Avg clusters per timestamp: {valid_quality['Unique_Clusters'].mean():.1f}\")\n",
    "    print(f\"  Avg noise percentage: {valid_quality['Noise_Pct'].mean():.1%}\")\n",
    "    print(f\"  Avg PCA variance retained: {valid_quality['Variance_Explained'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nCluster History Generated:\")\n",
    "print(f\"  Total rows: {len(cluster_history)}\")\n",
    "print(f\"  Unique timestamps: {cluster_history['Datetime'].nunique()}\")\n",
    "print(f\"  Date range: {cluster_history['Datetime'].min()} to {cluster_history['Datetime'].max()}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Cluster Stability Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Analyzing Cluster Stability\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "pair_co_cluster_freq = {}\n",
    "\n",
    "for ts in cluster_history['Datetime'].unique():\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    \n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        \n",
    "        members = snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist()\n",
    "        \n",
    "        for s1, s2 in itertools.combinations(sorted(members), 2):\n",
    "            pair_key = (s1, s2)\n",
    "            \n",
    "            if pair_key not in pair_co_cluster_freq:\n",
    "                pair_co_cluster_freq[pair_key] = 0\n",
    "            pair_co_cluster_freq[pair_key] += 1\n",
    "\n",
    "# Calculate frequencies\n",
    "total_valid_windows = cluster_history['Datetime'].nunique()\n",
    "\n",
    "pair_stability_data = []\n",
    "for pair, count in pair_co_cluster_freq.items():\n",
    "    frequency = count / total_valid_windows\n",
    "    pair_stability_data.append({\n",
    "        'Ticker_1': pair[0],\n",
    "        'Ticker_2': pair[1],\n",
    "        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "        'Co_Cluster_Count': count,\n",
    "        'Co_Cluster_Frequency': frequency,\n",
    "        'Is_Stable': frequency > 0.30\n",
    "    })\n",
    "\n",
    "df_pair_stability = pd.DataFrame(pair_stability_data).sort_values('Co_Cluster_Frequency', ascending=False)\n",
    "\n",
    "print(f\"\\nPair Clustering Analysis:\")\n",
    "print(f\"  Total unique pairs observed: {len(df_pair_stability)}\")\n",
    "print(f\"  Pairs clustered together >50% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.50).sum()}\")\n",
    "print(f\"  Pairs clustered together >30% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.30).sum()}\")\n",
    "print(f\"  Pairs clustered together <10% of time: {(df_pair_stability['Co_Cluster_Frequency'] < 0.10).sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(df_pair_stability[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\")\n",
    "print(f\"{'='*80}\")\n",
    "transient_pairs = df_pair_stability[\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] > 0.05) &\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] < 0.20)\n",
    "].sort_values('Co_Cluster_Frequency', ascending=True)\n",
    "print(transient_pairs[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: Temporal Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 3: Temporal Analysis - When Do Regimes Shift?\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_history['Date'] = pd.to_datetime(cluster_history['Datetime']).dt.date\n",
    "\n",
    "daily_cluster_stats = cluster_history.groupby('Date').agg({\n",
    "    'Cluster_ID': lambda x: len(set(x)) - (1 if -1 in x.values else 0),\n",
    "    'Ticker': 'count'\n",
    "}).rename(columns={'Cluster_ID': 'Num_Clusters', 'Ticker': 'Total_Obs'})\n",
    "\n",
    "print(f\"\\nDaily Clustering Variability:\")\n",
    "print(f\"  Days with high differentiation (>4 clusters): {(daily_cluster_stats['Num_Clusters'] > 4).sum()}\")\n",
    "print(f\"  Days with low differentiation (â‰¤2 clusters): {(daily_cluster_stats['Num_Clusters'] <= 2).sum()}\")\n",
    "\n",
    "mean_clusters = daily_cluster_stats['Num_Clusters'].mean()\n",
    "std_clusters = daily_cluster_stats['Num_Clusters'].std()\n",
    "regime_shift_days = daily_cluster_stats[\n",
    "    abs(daily_cluster_stats['Num_Clusters'] - mean_clusters) > 1.5 * std_clusters\n",
    "]\n",
    "\n",
    "if len(regime_shift_days) > 0:\n",
    "    print(f\"\\nPotential Regime Shift Days (unusual cluster patterns):\")\n",
    "    print(f\"  {len(regime_shift_days)} days detected\")\n",
    "    print(f\"\\nTop 5 Most Unusual Days:\")\n",
    "    top_shifts = regime_shift_days.nlargest(5, 'Num_Clusters')\n",
    "    for date, row in top_shifts.iterrows():\n",
    "        print(f\"  {date}: {row['Num_Clusters']:.0f} clusters (avg: {mean_clusters:.1f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLUSTERING PHASE COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nData structures created:\")\n",
    "print(f\"  - cluster_history: {len(cluster_history)} rows\")\n",
    "print(f\"  - df_quality: {len(df_quality)} rows\")\n",
    "print(f\"  - df_pair_stability: {len(df_pair_stability)} rows\")\n",
    "print(f\"\\nReady for pair testing phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bc21b",
   "metadata": {},
   "source": [
    "# Validation Testing Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
