{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c551ab07",
   "metadata": {},
   "source": [
    "#   Optics Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa0dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import OPTICS\n",
    "from statsmodels.tsa.stattools import coint, grangercausalitytests\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e12cae",
   "metadata": {},
   "source": [
    "# Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a64ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  41 of 41 completed\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "stocks = [\n",
    "    # S&P for Beta\n",
    "    \"^GSPC\",\n",
    "    # Megacap Leaders & Generalists\n",
    "    \"NVDA\", \"TSM\", \"AVGO\", \"AMD\", \"INTC\", \"MU\", \"TXN\", \"QCOM\", \"ADI\", \"MCHP\",\n",
    "    \n",
    "    # Equipment & Manufacturing\n",
    "    \"ASML\", \"AMAT\", \"LRCX\", \"KLAC\", \"TER\", \"ENTG\", \"NVMI\", \"TOELY\",\n",
    "    \n",
    "    # Specialized\n",
    "    \"ON\", \"NXPI\", \"STM\", \"LSCC\", \"MPWR\", \"QRVO\", \"SWKS\", \"ALAB\", \"CRDO\",\n",
    "    \n",
    "    # Intellectual Property & Design Software\n",
    "    \"ARM\", \"SNPS\", \"CDNS\", \"CEVA\",\n",
    "    \n",
    "    # Memory & Storage\n",
    "    \"WDC\", \"STX\", # Removed extra \"MU\" here\n",
    "    \n",
    "    # Emerging & Mid-Cap\n",
    "    \"GFS\", \"MRVL\", \"MTSI\", \"POWI\", \"SMTC\", \"VICR\", \"CAMT\"\n",
    "]\n",
    "\n",
    "def fetch_data(stocks):\n",
    "    data = yf.download(tickers=stocks, period=\"252d\", interval=\"1h\", group_by='ticker', auto_adjust=True, threads=True)\n",
    "    \n",
    "    price_series_list = []\n",
    "    for s in stocks:\n",
    "        try: \n",
    "            if s in data:\n",
    "                series = data[s]['Close']\n",
    "                series.name = s\n",
    "                price_series_list.append(series)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if price_series_list:\n",
    "        df = pd.concat(price_series_list, axis=1)\n",
    "        # Market-hours-aware fill: only forward-fill within the same trading day\n",
    "        # to avoid creating false correlations across overnight/weekend gaps\n",
    "        df = df.groupby(df.index.date).apply(lambda g: g.ffill()).droplevel(0)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df = fetch_data(stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae0f61",
   "metadata": {},
   "source": [
    "## Factor Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b24f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\n",
      "================================================================================\n",
      "Short-term window: 50 hours (~1 week)\n",
      "Medium-term window: 147 hours (~3 weeks)\n",
      "Optimizing for transient events: 10-50 hour duration\n",
      "\n",
      "Calculating SHORT-TERM features (primary regime indicators)...\n",
      "Calculating MEDIUM-TERM features (context indicators)...\n",
      "Calculating INSTANTANEOUS features (momentum indicators)...\n",
      "Calculating REGIME CHANGE indicators...\n",
      "\n",
      "Assembling time-series dataframe...\n",
      "\n",
      "================================================================================\n",
      "TIME-SERIES DATAFRAME CREATED SUCCESSFULLY\n",
      "================================================================================\n",
      "Total rows: 63,021\n",
      "Rows dropped (NaN): 7,179 (10.2%)\n",
      "Date range: 2025-03-11 13:30:00+00:00 to 2026-02-09 20:30:00+00:00\n",
      "Unique tickers: 40\n",
      "\n",
      "Feature columns: 13\n",
      "\n",
      "Sample data:\n",
      "                                       Price   Returns  Vol_Short  \\\n",
      "Datetime                  Ticker                                    \n",
      "2025-03-11 13:30:00+00:00 NVDA    106.309998 -0.006634   0.015222   \n",
      "2025-03-11 14:30:00+00:00 NVDA    108.894997  0.024316   0.015279   \n",
      "2025-03-11 15:30:00+00:00 NVDA    108.820000 -0.000689   0.015244   \n",
      "2025-03-11 16:30:00+00:00 NVDA    107.559998 -0.011579   0.015266   \n",
      "2025-03-11 17:30:00+00:00 NVDA    110.430000  0.026683   0.015728   \n",
      "\n",
      "                                  Beta_SPX_Short  Beta_Sector_Short  \\\n",
      "Datetime                  Ticker                                      \n",
      "2025-03-11 13:30:00+00:00 NVDA          2.015452           1.047482   \n",
      "2025-03-11 14:30:00+00:00 NVDA          2.010241           1.073785   \n",
      "2025-03-11 15:30:00+00:00 NVDA          2.004732           1.077785   \n",
      "2025-03-11 16:30:00+00:00 NVDA          1.996936           1.077607   \n",
      "2025-03-11 17:30:00+00:00 NVDA          2.013671           1.086206   \n",
      "\n",
      "                                  Vol_Medium  Beta_SPX_Medium  \\\n",
      "Datetime                  Ticker                                \n",
      "2025-03-11 13:30:00+00:00 NVDA      0.012478         2.258875   \n",
      "2025-03-11 14:30:00+00:00 NVDA      0.012654         2.287974   \n",
      "2025-03-11 15:30:00+00:00 NVDA      0.012652         2.284791   \n",
      "2025-03-11 16:30:00+00:00 NVDA      0.012660         2.275428   \n",
      "2025-03-11 17:30:00+00:00 NVDA      0.012851         2.279881   \n",
      "\n",
      "                                  Beta_Sector_Medium        RSI  Momentum_5H  \\\n",
      "Datetime                  Ticker                                               \n",
      "2025-03-11 13:30:00+00:00 NVDA              1.125397  40.105059    -0.016422   \n",
      "2025-03-11 14:30:00+00:00 NVDA              1.143584  41.166575     0.017139   \n",
      "2025-03-11 15:30:00+00:00 NVDA              1.143382  39.697839     0.019296   \n",
      "2025-03-11 16:30:00+00:00 NVDA              1.143350  39.244395     0.008533   \n",
      "2025-03-11 17:30:00+00:00 NVDA              1.144117  41.452668     0.031863   \n",
      "\n",
      "                                  Momentum_10H  Momentum_Accel  \\\n",
      "Datetime                  Ticker                                 \n",
      "2025-03-11 13:30:00+00:00 NVDA       -0.052918        0.020682   \n",
      "2025-03-11 14:30:00+00:00 NVDA       -0.032947        0.066381   \n",
      "2025-03-11 15:30:00+00:00 NVDA       -0.034685        0.072254   \n",
      "2025-03-11 16:30:00+00:00 NVDA       -0.004668        0.021623   \n",
      "2025-03-11 17:30:00+00:00 NVDA        0.025746        0.037791   \n",
      "\n",
      "                                  Vol_Regime_Shift  Beta_SPX_Regime_Shift  \\\n",
      "Datetime                  Ticker                                            \n",
      "2025-03-11 13:30:00+00:00 NVDA            0.219828              -0.243422   \n",
      "2025-03-11 14:30:00+00:00 NVDA            0.207400              -0.277733   \n",
      "2025-03-11 15:30:00+00:00 NVDA            0.204832              -0.280059   \n",
      "2025-03-11 16:30:00+00:00 NVDA            0.205831              -0.278492   \n",
      "2025-03-11 17:30:00+00:00 NVDA            0.223904              -0.266210   \n",
      "\n",
      "                                  Beta_Sector_Regime_Shift  \n",
      "Datetime                  Ticker                            \n",
      "2025-03-11 13:30:00+00:00 NVDA                   -0.077915  \n",
      "2025-03-11 14:30:00+00:00 NVDA                   -0.069798  \n",
      "2025-03-11 15:30:00+00:00 NVDA                   -0.065597  \n",
      "2025-03-11 16:30:00+00:00 NVDA                   -0.065743  \n",
      "2025-03-11 17:30:00+00:00 NVDA                   -0.057912  \n",
      "\n",
      "================================================================================\n",
      "SKIPPING STATIC FUNDAMENTALS (Not relevant for transient detection)\n",
      "================================================================================\n",
      "Transient coupling is driven by events/market dynamics, not fundamental profiles.\n",
      "If you want to filter pairs by fundamentals later, re-enable this section.\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETE - Ready for clustering\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING FOR TRANSIENT REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Clean and Prepare Price Data\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    if 'Close' in df.columns.get_level_values(0):\n",
    "        df = df['Close']\n",
    "    elif 'Close' in df.columns.get_level_values(1):\n",
    "        df = df.xs('Close', axis=1, level=1)\n",
    "\n",
    "# 2. Base Calculations\n",
    "returns_df = df.pct_change().dropna()\n",
    "market_returns = returns_df['^GSPC']\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL CHANGE: Multi-Timeframe Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "# SHORT-TERM WINDOW (Transient regime detection)\n",
    "window_short = 50  # ~1 week of hourly data - ALIGNED WITH TRADE DURATION\n",
    "\n",
    "# MEDIUM-TERM WINDOW (Context/stability check)\n",
    "window_medium = 147  # ~3 weeks - your original window\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Short-term window: {window_short} hours (~1 week)\")\n",
    "print(f\"Medium-term window: {window_medium} hours (~3 weeks)\")\n",
    "print(f\"Optimizing for transient events: 10-50 hour duration\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3A. SHORT-TERM FEATURES (Primary clustering features)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating SHORT-TERM features (primary regime indicators)...\")\n",
    "\n",
    "# Feature A: SHORT-TERM Volatility (Recent risk behavior)\n",
    "rolling_vol_short = returns_df.rolling(window=window_short).std()\n",
    "\n",
    "# Feature B: SHORT-TERM Beta to SPX (Recent market sensitivity)\n",
    "rolling_cov_mkt_short = returns_df.rolling(window=window_short).cov(market_returns)\n",
    "rolling_mkt_var_short = market_returns.rolling(window=window_short).var()\n",
    "rolling_beta_spx_short = rolling_cov_mkt_short.divide(rolling_mkt_var_short, axis=0)\n",
    "\n",
    "# Feature C: SHORT-TERM Beta to Sector (Recent sector coupling)\n",
    "non_spx_returns = returns_df.drop(columns=['^GSPC'], errors='ignore').dropna(axis=1, how='all')\n",
    "sector_sum = non_spx_returns.sum(axis=1)\n",
    "n_stocks = non_spx_returns.count(axis=1)\n",
    "\n",
    "# Leave-one-out sector average and rolling beta for each ticker\n",
    "rolling_beta_sector_short = pd.DataFrame(index=returns_df.index, columns=non_spx_returns.columns)\n",
    "for ticker in non_spx_returns.columns:\n",
    "    loo_sector = (sector_sum - non_spx_returns[ticker].fillna(0)) / (n_stocks - 1).clip(lower=1)\n",
    "    cov = returns_df[ticker].rolling(window=window_short).cov(loo_sector)\n",
    "    var = loo_sector.rolling(window=window_short).var()\n",
    "    rolling_beta_sector_short[ticker] = cov / var\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3B. MEDIUM-TERM FEATURES (Context/stability indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating MEDIUM-TERM features (context indicators)...\")\n",
    "\n",
    "# These help identify if current behavior is unusual vs. longer-term baseline\n",
    "rolling_vol_medium = returns_df.rolling(window=window_medium).std()\n",
    "\n",
    "rolling_cov_mkt_medium = returns_df.rolling(window=window_medium).cov(market_returns)\n",
    "rolling_mkt_var_medium = market_returns.rolling(window=window_medium).var()\n",
    "rolling_beta_spx_medium = rolling_cov_mkt_medium.divide(rolling_mkt_var_medium, axis=0)\n",
    "\n",
    "rolling_beta_sector_medium = pd.DataFrame(index=returns_df.index, columns=non_spx_returns.columns)\n",
    "for ticker in non_spx_returns.columns:\n",
    "    loo_sector = (sector_sum - non_spx_returns[ticker].fillna(0)) / (n_stocks - 1).clip(lower=1)\n",
    "    cov = returns_df[ticker].rolling(window=window_medium).cov(loo_sector)\n",
    "    var = loo_sector.rolling(window=window_medium).var()\n",
    "    rolling_beta_sector_medium[ticker] = cov / var\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3C. INSTANTANEOUS FEATURES (Momentum/Overbought indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating INSTANTANEOUS features (momentum indicators)...\")\n",
    "\n",
    "# Feature D: RSI (Momentum/Overextended) - 70 periods for hourly data (~2 weeks)\n",
    "def calculate_rsi(data, window=70):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi_df = df.apply(calculate_rsi)\n",
    "\n",
    "# Feature E: Short Term Momentum (5-period return)\n",
    "momentum_5h = df.pct_change(periods=5)\n",
    "\n",
    "# Feature F: Momentum Acceleration (change in momentum)\n",
    "momentum_10h = df.pct_change(periods=10)\n",
    "momentum_acceleration = momentum_5h - momentum_5h.shift(5)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3D. REGIME CHANGE INDICATORS (New!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating REGIME CHANGE indicators...\")\n",
    "\n",
    "# Detect when short-term behavior diverges from medium-term baseline\n",
    "# This helps identify when a NEW regime is forming\n",
    "\n",
    "# Volatility Regime Shift (is vol spiking vs. baseline?)\n",
    "vol_regime_shift = (rolling_vol_short - rolling_vol_medium) / rolling_vol_medium\n",
    "\n",
    "# Beta Regime Shift (is market sensitivity changing?)\n",
    "beta_spx_regime_shift = rolling_beta_spx_short - rolling_beta_spx_medium\n",
    "beta_sector_regime_shift = rolling_beta_sector_short - rolling_beta_sector_medium\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Assemble the Master Time-Series DataFrame (ts_df)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nAssembling time-series dataframe...\")\n",
    "\n",
    "ts_data_list = []\n",
    "\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC' or ticker not in df.columns: \n",
    "        continue\n",
    "    \n",
    "    # Extract features for this specific ticker\n",
    "    temp_df = pd.DataFrame({\n",
    "        # Price & Returns (baseline)\n",
    "        'Price': df[ticker],\n",
    "        'Returns': returns_df[ticker],\n",
    "        \n",
    "        # SHORT-TERM FEATURES (Primary clustering features)\n",
    "        'Vol_Short': rolling_vol_short[ticker],\n",
    "        'Beta_SPX_Short': rolling_beta_spx_short[ticker],\n",
    "        'Beta_Sector_Short': rolling_beta_sector_short[ticker],\n",
    "        \n",
    "        # MEDIUM-TERM FEATURES (Context)\n",
    "        'Vol_Medium': rolling_vol_medium[ticker],\n",
    "        'Beta_SPX_Medium': rolling_beta_spx_medium[ticker],\n",
    "        'Beta_Sector_Medium': rolling_beta_sector_medium[ticker],\n",
    "        \n",
    "        # INSTANTANEOUS FEATURES\n",
    "        'RSI': rsi_df[ticker],\n",
    "        'Momentum_5H': momentum_5h[ticker],\n",
    "        'Momentum_10H': momentum_10h[ticker],\n",
    "        'Momentum_Accel': momentum_acceleration[ticker],\n",
    "        \n",
    "        # REGIME CHANGE INDICATORS (New!)\n",
    "        'Vol_Regime_Shift': vol_regime_shift[ticker],\n",
    "        'Beta_SPX_Regime_Shift': beta_spx_regime_shift[ticker],\n",
    "        'Beta_Sector_Regime_Shift': beta_sector_regime_shift[ticker],\n",
    "        \n",
    "    }, index=df.index)\n",
    "    \n",
    "    temp_df['Ticker'] = ticker\n",
    "    ts_data_list.append(temp_df)\n",
    "\n",
    "if ts_data_list:\n",
    "    ts_df = pd.concat(ts_data_list).reset_index().set_index(['Datetime', 'Ticker'])\n",
    "    \n",
    "    # Drop NaNs created by rolling windows\n",
    "    initial_rows = len(ts_df)\n",
    "    ts_df = ts_df.dropna()\n",
    "    dropped_rows = initial_rows - len(ts_df)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TIME-SERIES DATAFRAME CREATED SUCCESSFULLY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(ts_df):,}\")\n",
    "    print(f\"Rows dropped (NaN): {dropped_rows:,} ({dropped_rows/initial_rows:.1%})\")\n",
    "    print(f\"Date range: {ts_df.index.get_level_values('Datetime').min()} to {ts_df.index.get_level_values('Datetime').max()}\")\n",
    "    print(f\"Unique tickers: {ts_df.index.get_level_values('Ticker').nunique()}\")\n",
    "    print(f\"\\nFeature columns: {len([c for c in ts_df.columns if c not in ['Price', 'Returns', 'Ticker']])}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(ts_df.head())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. OPTIONAL: Static Fundamental DataFrame (Keep or Remove?)\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: For transient regime detection, fundamentals are less relevant\n",
    "# Transient coupling is driven by events/news, not fundamental similarity\n",
    "# Consider REMOVING this section unless you plan to use it for filtering\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SKIPPING STATIC FUNDAMENTALS (Not relevant for transient detection)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"Transient coupling is driven by events/market dynamics, not fundamental profiles.\")\n",
    "print(\"If you want to filter pairs by fundamentals later, re-enable this section.\\n\")\n",
    "\n",
    "# Uncomment below if you want to keep fundamentals\n",
    "\"\"\"\n",
    "fundamental_list = []\n",
    "print(\"Fetching Static Fundamentals...\")\n",
    "\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC': continue\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        info = t.info\n",
    "        \n",
    "        fundamental_list.append({\n",
    "            'Ticker': ticker,\n",
    "            'Sector': info.get('sector', 'Unknown'),\n",
    "            'Industry': info.get('industry', 'Unknown'),\n",
    "            'Market_Cap': info.get('marketCap', np.nan),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "        continue\n",
    "\n",
    "static_df = pd.DataFrame(fundamental_list).set_index('Ticker')\n",
    "print(\"Static DataFrame (static_df) Created Successfully!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING COMPLETE - Ready for clustering\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf953a66",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## Based on an Hourly Time Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ab02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSIENT REGIME DETECTION - OPTICS CLUSTERING\n",
      "================================================================================\n",
      "Data Density: 1583 valid hourly timestamps\n",
      "Date Range: 2025-03-11 13:30:00+00:00 to 2026-02-09 20:30:00+00:00\n",
      "\n",
      "Using features: ['Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short', 'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift', 'Beta_Sector_Regime_Shift']\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Running OPTICS Clustering (Hourly Snapshots)\n",
      "================================================================================\n",
      "  Processed 100/1583 timestamps... (77 valid so far)\n",
      "  Processed 200/1583 timestamps... (171 valid so far)\n",
      "  Processed 300/1583 timestamps... (263 valid so far)\n",
      "  Processed 400/1583 timestamps... (337 valid so far)\n",
      "  Processed 500/1583 timestamps... (428 valid so far)\n",
      "  Processed 600/1583 timestamps... (520 valid so far)\n",
      "  Processed 700/1583 timestamps... (600 valid so far)\n",
      "  Processed 800/1583 timestamps... (692 valid so far)\n",
      "  Processed 900/1583 timestamps... (781 valid so far)\n",
      "  Processed 1000/1583 timestamps... (858 valid so far)\n",
      "  Processed 1100/1583 timestamps... (936 valid so far)\n",
      "  Processed 1200/1583 timestamps... (1026 valid so far)\n",
      "  Processed 1300/1583 timestamps... (1117 valid so far)\n",
      "  Processed 1400/1583 timestamps... (1199 valid so far)\n",
      "  Processed 1500/1583 timestamps... (1293 valid so far)\n",
      "\n",
      "================================================================================\n",
      "PHASE 1 COMPLETE: Cluster Quality Summary\n",
      "================================================================================\n",
      "\n",
      "Timestamp Analysis:\n",
      "  Total timestamps processed: 1583\n",
      "  Valid clustering windows: 1368 (86.4%)\n",
      "  Invalid/skipped windows: 215 (13.6%)\n",
      "\n",
      "Skip Reasons:\n",
      "  - Too noisy (77.5% noise): 65 (30.2%)\n",
      "  - Too noisy (80.0% noise): 43 (20.0%)\n",
      "  - Too noisy (82.5% noise): 42 (19.5%)\n",
      "  - Too noisy (85.0% noise): 28 (13.0%)\n",
      "  - Too noisy (90.0% noise): 12 (5.6%)\n",
      "  - Too noisy (87.5% noise): 11 (5.1%)\n",
      "  - Too noisy (92.5% noise): 10 (4.7%)\n",
      "  - Too noisy (84.6% noise): 2 (0.9%)\n",
      "  - Too noisy (76.9% noise): 1 (0.5%)\n",
      "  - Too noisy (79.5% noise): 1 (0.5%)\n",
      "\n",
      "Valid Cluster Statistics:\n",
      "  Avg clusters per timestamp: 3.3\n",
      "  Avg noise percentage: 58.2%\n",
      "  Avg PCA variance retained: 92.9%\n",
      "\n",
      "Cluster History Generated:\n",
      "  Total rows: 54425\n",
      "  Unique timestamps: 1368\n",
      "  Date range: 2025-03-11 13:30:00+00:00 to 2026-02-09 20:30:00+00:00\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Analyzing Cluster Stability\n",
      "================================================================================\n",
      "\n",
      "Pair Clustering Analysis:\n",
      "  Total unique pairs observed: 775\n",
      "  Pairs clustered together >50% of time: 0\n",
      "  Pairs clustered together >30% of time: 4\n",
      "  Pairs clustered together <10% of time: 684\n",
      "\n",
      "================================================================================\n",
      "TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\n",
      "================================================================================\n",
      "     Pair  Co_Cluster_Count  Co_Cluster_Frequency\n",
      "AMAT-LRCX               509              0.372076\n",
      "QRVO-SWKS               508              0.371345\n",
      "KLAC-LRCX               507              0.370614\n",
      "AMAT-KLAC               462              0.337719\n",
      " ADI-NXPI               391              0.285819\n",
      "  ADI-TXN               372              0.271930\n",
      "MCHP-NXPI               323              0.236111\n",
      "QCOM-QRVO               315              0.230263\n",
      " NXPI-TXN               307              0.224415\n",
      " ADI-SWKS               305              0.222953\n",
      "QCOM-SWKS               301              0.220029\n",
      " NXPI-STM               281              0.205409\n",
      "ASML-KLAC               279              0.203947\n",
      "  MCHP-ON               269              0.196637\n",
      "CDNS-SNPS               264              0.192982\n",
      "\n",
      "================================================================================\n",
      "TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\n",
      "================================================================================\n",
      "     Pair  Co_Cluster_Count  Co_Cluster_Frequency\n",
      "  TER-WDC                69              0.050439\n",
      "CDNS-KLAC                69              0.050439\n",
      " AMD-MPWR                69              0.050439\n",
      "  STX-TER                69              0.050439\n",
      " MTSI-TER                70              0.051170\n",
      "NVDA-NXPI                70              0.051170\n",
      " CDNS-GFS                70              0.051170\n",
      " CEVA-GFS                70              0.051170\n",
      "MRVL-SMTC                70              0.051170\n",
      " KLAC-WDC                71              0.051901\n",
      "MPWR-NVMI                71              0.051901\n",
      "ASML-ENTG                71              0.051901\n",
      "CAMT-MRVL                72              0.052632\n",
      " ARM-NVDA                72              0.052632\n",
      " AMD-AVGO                72              0.052632\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: Temporal Analysis - When Do Regimes Shift?\n",
      "================================================================================\n",
      "\n",
      "Daily Clustering Variability:\n",
      "  Days with high differentiation (>4 clusters): 117\n",
      "  Days with low differentiation (≤2 clusters): 2\n",
      "\n",
      "Potential Regime Shift Days (unusual cluster patterns):\n",
      "  52 days detected\n",
      "\n",
      "Top 5 Most Unusual Days:\n",
      "  2025-06-18: 7 clusters (avg: 4.5)\n",
      "  2025-10-13: 7 clusters (avg: 4.5)\n",
      "  2025-04-04: 6 clusters (avg: 4.5)\n",
      "  2025-04-08: 6 clusters (avg: 4.5)\n",
      "  2025-04-11: 6 clusters (avg: 4.5)\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING PHASE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Data structures created:\n",
      "  - cluster_history: 54425 rows\n",
      "  - df_quality: 1583 rows\n",
      "  - df_pair_stability: 775 rows\n",
      "\n",
      "Ready for pair testing phase\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTICS CLUSTERING FOR TRANSIENT REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "if 'ts_df' not in locals():\n",
    "    raise ValueError(\"Please run the Feature Engineering cell to create 'ts_df' first.\")\n",
    "\n",
    "# Clean duplicates\n",
    "ts_df = ts_df[~ts_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Check Density\n",
    "density = ts_df.groupby(level='Datetime').size()\n",
    "valid_timestamps = density[density >= 5].index\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRANSIENT REGIME DETECTION - OPTICS CLUSTERING\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Data Density: {len(valid_timestamps)} valid hourly timestamps\")\n",
    "print(f\"Date Range: {valid_timestamps.min()} to {valid_timestamps.max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURES: Short-term + regime shift indicators for transient detection\n",
    "# ============================================================================\n",
    "features_to_cluster = ['Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short', 'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift', 'Beta_Sector_Regime_Shift']\n",
    "print(f\"\\nUsing features: {features_to_cluster}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLUSTERING LOOP - Hourly Regime Detection\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1: Running OPTICS Clustering (Hourly Snapshots)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_results = []\n",
    "cluster_quality_log = []\n",
    "\n",
    "for i, ts in enumerate(valid_timestamps):\n",
    "    try:\n",
    "        snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "        if len(snapshot) < 5: \n",
    "            continue\n",
    "        \n",
    "        # Scale & PCA (Dimensionality Reduction)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(snapshot)\n",
    "        \n",
    "        pca = PCA(n_components=0.90)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # OPTICS Clustering\n",
    "        optics = OPTICS(min_samples=3, metric='euclidean', xi=0.05, min_cluster_size=3)\n",
    "        optics.fit(X_pca)\n",
    "        \n",
    "        # ===== CLUSTER QUALITY VALIDATION (RELAXED) =====\n",
    "        unique_clusters = len(set(optics.labels_)) - (1 if -1 in optics.labels_ else 0)\n",
    "        noise_count = (optics.labels_ == -1).sum()\n",
    "        noise_pct = noise_count / len(optics.labels_)\n",
    "        total_stocks = len(optics.labels_)\n",
    "        \n",
    "        # Quality Metrics\n",
    "        quality_metrics = {\n",
    "            'Datetime': ts,\n",
    "            'Total_Stocks': total_stocks,\n",
    "            'Unique_Clusters': unique_clusters,\n",
    "            'Noise_Count': noise_count,\n",
    "            'Noise_Pct': noise_pct,\n",
    "            'PCA_Components': X_pca.shape[1],\n",
    "            'Variance_Explained': pca.explained_variance_ratio_.sum()\n",
    "        }\n",
    "        \n",
    "        # RELAXED Quality Filters\n",
    "        is_valid = True\n",
    "        skip_reason = None\n",
    "        \n",
    "        if unique_clusters < 1:\n",
    "            is_valid = False\n",
    "            skip_reason = \"No clusters found (all noise)\"\n",
    "        elif noise_pct > 0.75:  # Relaxed to 75%\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Too noisy ({noise_pct:.1%} noise)\"\n",
    "        elif unique_clusters > total_stocks * 0.75:\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Over-fragmented ({unique_clusters} clusters for {total_stocks} stocks)\"\n",
    "        \n",
    "        quality_metrics['Is_Valid'] = is_valid\n",
    "        quality_metrics['Skip_Reason'] = skip_reason\n",
    "        cluster_quality_log.append(quality_metrics)\n",
    "        \n",
    "        # Store valid clusters\n",
    "        if is_valid:\n",
    "            snapshot['Cluster_ID'] = optics.labels_\n",
    "            snapshot['Datetime'] = ts\n",
    "            snapshot['Num_Clusters'] = unique_clusters\n",
    "            snapshot['Noise_Pct'] = noise_pct\n",
    "            cluster_results.append(snapshot.reset_index())\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 100 == 0:\n",
    "            valid_so_far = len(cluster_results)\n",
    "            print(f\"  Processed {i+1}/{len(valid_timestamps)} timestamps... ({valid_so_far} valid so far)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        cluster_quality_log.append({\n",
    "            'Datetime': ts,\n",
    "            'Total_Stocks': np.nan,\n",
    "            'Unique_Clusters': np.nan,\n",
    "            'Noise_Count': np.nan,\n",
    "            'Noise_Pct': np.nan,\n",
    "            'PCA_Components': np.nan,\n",
    "            'Variance_Explained': np.nan,\n",
    "            'Is_Valid': False,\n",
    "            'Skip_Reason': f\"Error: {str(e)[:50]}\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "if not cluster_results:\n",
    "    raise ValueError(\"No valid clusters found. Check your data quality and OPTICS parameters.\")\n",
    "\n",
    "cluster_history = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1 COMPLETE: Cluster Quality Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLUSTER QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "df_quality = pd.DataFrame(cluster_quality_log)\n",
    "\n",
    "total_timestamps = len(df_quality)\n",
    "valid_timestamps_count = df_quality['Is_Valid'].sum()\n",
    "invalid_timestamps_count = total_timestamps - valid_timestamps_count\n",
    "\n",
    "print(f\"\\nTimestamp Analysis:\")\n",
    "print(f\"  Total timestamps processed: {total_timestamps}\")\n",
    "print(f\"  Valid clustering windows: {valid_timestamps_count} ({valid_timestamps_count/total_timestamps:.1%})\")\n",
    "print(f\"  Invalid/skipped windows: {invalid_timestamps_count} ({invalid_timestamps_count/total_timestamps:.1%})\")\n",
    "\n",
    "if invalid_timestamps_count > 0:\n",
    "    print(f\"\\nSkip Reasons:\")\n",
    "    skip_summary = df_quality[~df_quality['Is_Valid']]['Skip_Reason'].value_counts()\n",
    "    for reason, count in skip_summary.items():\n",
    "        print(f\"  - {reason}: {count} ({count/invalid_timestamps_count:.1%})\")\n",
    "\n",
    "# Valid cluster statistics\n",
    "valid_quality = df_quality[df_quality['Is_Valid']]\n",
    "if len(valid_quality) > 0:\n",
    "    print(f\"\\nValid Cluster Statistics:\")\n",
    "    print(f\"  Avg clusters per timestamp: {valid_quality['Unique_Clusters'].mean():.1f}\")\n",
    "    print(f\"  Avg noise percentage: {valid_quality['Noise_Pct'].mean():.1%}\")\n",
    "    print(f\"  Avg PCA variance retained: {valid_quality['Variance_Explained'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nCluster History Generated:\")\n",
    "print(f\"  Total rows: {len(cluster_history)}\")\n",
    "print(f\"  Unique timestamps: {cluster_history['Datetime'].nunique()}\")\n",
    "print(f\"  Date range: {cluster_history['Datetime'].min()} to {cluster_history['Datetime'].max()}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Cluster Stability Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Analyzing Cluster Stability\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "pair_co_cluster_freq = {}\n",
    "\n",
    "for ts in cluster_history['Datetime'].unique():\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    \n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        \n",
    "        members = snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist()\n",
    "        \n",
    "        for s1, s2 in itertools.combinations(sorted(members), 2):\n",
    "            pair_key = (s1, s2)\n",
    "            \n",
    "            if pair_key not in pair_co_cluster_freq:\n",
    "                pair_co_cluster_freq[pair_key] = 0\n",
    "            pair_co_cluster_freq[pair_key] += 1\n",
    "\n",
    "# Calculate frequencies\n",
    "total_valid_windows = cluster_history['Datetime'].nunique()\n",
    "\n",
    "pair_stability_data = []\n",
    "for pair, count in pair_co_cluster_freq.items():\n",
    "    frequency = count / total_valid_windows\n",
    "    pair_stability_data.append({\n",
    "        'Ticker_1': pair[0],\n",
    "        'Ticker_2': pair[1],\n",
    "        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "        'Co_Cluster_Count': count,\n",
    "        'Co_Cluster_Frequency': frequency,\n",
    "        'Is_Stable': frequency > 0.25\n",
    "    })\n",
    "\n",
    "df_pair_stability = pd.DataFrame(pair_stability_data).sort_values('Co_Cluster_Frequency', ascending=False)\n",
    "\n",
    "print(f\"\\nPair Clustering Analysis:\")\n",
    "print(f\"  Total unique pairs observed: {len(df_pair_stability)}\")\n",
    "print(f\"  Pairs clustered together >50% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.50).sum()}\")\n",
    "print(f\"  Pairs clustered together >30% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.30).sum()}\")\n",
    "print(f\"  Pairs clustered together <10% of time: {(df_pair_stability['Co_Cluster_Frequency'] < 0.10).sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(df_pair_stability[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\")\n",
    "print(f\"{'='*80}\")\n",
    "transient_pairs = df_pair_stability[\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] > 0.05) &\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] < 0.20)\n",
    "].sort_values('Co_Cluster_Frequency', ascending=True)\n",
    "print(transient_pairs[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: Temporal Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 3: Temporal Analysis - When Do Regimes Shift?\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_history['Date'] = pd.to_datetime(cluster_history['Datetime']).dt.date\n",
    "\n",
    "daily_cluster_stats = cluster_history.groupby('Date').agg({\n",
    "    'Cluster_ID': lambda x: len(set(x)) - (1 if -1 in x.values else 0),\n",
    "    'Ticker': 'count'\n",
    "}).rename(columns={'Cluster_ID': 'Num_Clusters', 'Ticker': 'Total_Obs'})\n",
    "\n",
    "print(f\"\\nDaily Clustering Variability:\")\n",
    "print(f\"  Days with high differentiation (>4 clusters): {(daily_cluster_stats['Num_Clusters'] > 4).sum()}\")\n",
    "print(f\"  Days with low differentiation (≤2 clusters): {(daily_cluster_stats['Num_Clusters'] <= 2).sum()}\")\n",
    "\n",
    "mean_clusters = daily_cluster_stats['Num_Clusters'].mean()\n",
    "std_clusters = daily_cluster_stats['Num_Clusters'].std()\n",
    "regime_shift_days = daily_cluster_stats[\n",
    "    abs(daily_cluster_stats['Num_Clusters'] - mean_clusters) > 1.5 * std_clusters\n",
    "]\n",
    "\n",
    "if len(regime_shift_days) > 0:\n",
    "    print(f\"\\nPotential Regime Shift Days (unusual cluster patterns):\")\n",
    "    print(f\"  {len(regime_shift_days)} days detected\")\n",
    "    print(f\"\\nTop 5 Most Unusual Days:\")\n",
    "    top_shifts = regime_shift_days.nlargest(5, 'Num_Clusters')\n",
    "    for date, row in top_shifts.iterrows():\n",
    "        print(f\"  {date}: {row['Num_Clusters']:.0f} clusters (avg: {mean_clusters:.1f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLUSTERING PHASE COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nData structures created:\")\n",
    "print(f\"  - cluster_history: {len(cluster_history)} rows\")\n",
    "print(f\"  - df_quality: {len(df_quality)} rows\")\n",
    "print(f\"  - df_pair_stability: {len(df_pair_stability)} rows\")\n",
    "print(f\"\\nReady for pair testing phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b954a92",
   "metadata": {},
   "source": [
    "# Cluster Formation & Duration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebf0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLUSTER FORMATION & DISSOLUTION EVENT DETECTION\n",
    "# ============================================================================\n",
    "# Instead of just counting co-clustering frequency, detect the MOMENTS when\n",
    "# pairs START and STOP co-clustering. These formation events are what we\n",
    "# want to predict.\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"PHASE 2B: CLUSTER FORMATION & DISSOLUTION EVENTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Build a per-pair, per-timestamp co-clustering indicator\n",
    "# ============================================================================\n",
    "\n",
    "# Get all unique pairs that ever co-clustered (from Phase 2)\n",
    "all_pairs = list(pair_co_cluster_freq.keys())\n",
    "all_timestamps = sorted(cluster_history['Datetime'].unique())\n",
    "\n",
    "print(f\"Tracking {len(all_pairs)} pairs across {len(all_timestamps)} timestamps...\")\n",
    "\n",
    "# Build co-clustering matrix: for each pair, 1 if co-clustered at that timestamp, 0 otherwise\n",
    "pair_coclustering = {}\n",
    "\n",
    "for ts in all_timestamps:\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    coclustered_at_ts = set()\n",
    "    \n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        members = sorted(snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist())\n",
    "        for s1, s2 in itertools.combinations(members, 2):\n",
    "            coclustered_at_ts.add((s1, s2))\n",
    "    \n",
    "    for pair in all_pairs:\n",
    "        if pair not in pair_coclustering:\n",
    "            pair_coclustering[pair] = []\n",
    "        pair_coclustering[pair].append(1 if pair in coclustered_at_ts else 0)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Detect Formation and Dissolution Events\n",
    "# ============================================================================\n",
    "# A formation event occurs when a pair transitions from NOT co-clustering\n",
    "# to co-clustering (0 -> 1 transition).\n",
    "# A dissolution event is the reverse (1 -> 0 transition).\n",
    "# We require a minimum gap (MIN_GAP_HOURS) of non-co-clustering before a\n",
    "# new formation to avoid counting brief flickers.\n",
    "\n",
    "MIN_GAP_HOURS = 5  # Minimum hours of non-co-clustering before counting a new formation\n",
    "\n",
    "formation_events = []\n",
    "dissolution_events = []\n",
    "cluster_durations = []\n",
    "\n",
    "for pair, series in pair_coclustering.items():\n",
    "    ts_series = list(zip(all_timestamps, series))\n",
    "    \n",
    "    in_cluster = False\n",
    "    formation_ts = None\n",
    "    formation_idx = None\n",
    "    gap_count = MIN_GAP_HOURS  # Start with full gap so first co-clustering counts as formation\n",
    "    \n",
    "    for i, (ts, val) in enumerate(ts_series):\n",
    "        if val == 1:\n",
    "            if not in_cluster and gap_count >= MIN_GAP_HOURS:\n",
    "                # Formation event: pair just started co-clustering after a sufficient gap\n",
    "                in_cluster = True\n",
    "                formation_ts = ts\n",
    "                formation_idx = i\n",
    "                formation_events.append({\n",
    "                    'Ticker_1': pair[0],\n",
    "                    'Ticker_2': pair[1],\n",
    "                    'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                    'Formation_Time': ts,\n",
    "                    'Timestamp_Index': i\n",
    "                })\n",
    "            elif not in_cluster:\n",
    "                # Still in the gap period, but co-clustering again\n",
    "                in_cluster = True\n",
    "                formation_ts = ts\n",
    "                formation_idx = i\n",
    "            gap_count = 0\n",
    "        else:\n",
    "            if in_cluster:\n",
    "                gap_count += 1\n",
    "                if gap_count >= MIN_GAP_HOURS:\n",
    "                    # Dissolution confirmed after sufficient gap\n",
    "                    # The last co-clustering timestamp was at index (i - gap_count)\n",
    "                    last_cocluster_idx = i - gap_count\n",
    "                    dissolution_ts = all_timestamps[min(last_cocluster_idx + 1, len(all_timestamps) - 1)]\n",
    "                    duration = max(1, last_cocluster_idx - formation_idx + 1) if formation_idx is not None else 1\n",
    "                    \n",
    "                    dissolution_events.append({\n",
    "                        'Ticker_1': pair[0],\n",
    "                        'Ticker_2': pair[1],\n",
    "                        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                        'Dissolution_Time': dissolution_ts,\n",
    "                        'Duration_Hours': duration\n",
    "                    })\n",
    "                    \n",
    "                    if formation_ts is not None:\n",
    "                        cluster_durations.append({\n",
    "                            'Ticker_1': pair[0],\n",
    "                            'Ticker_2': pair[1],\n",
    "                            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                            'Formation_Time': formation_ts,\n",
    "                            'Dissolution_Time': dissolution_ts,\n",
    "                            'Duration_Hours': duration\n",
    "                        })\n",
    "                    \n",
    "                    in_cluster = False\n",
    "                    formation_ts = None\n",
    "                    formation_idx = None\n",
    "            else:\n",
    "                gap_count += 1\n",
    "\n",
    "    # Handle pairs still in cluster at end of series\n",
    "    if in_cluster and formation_ts is not None:\n",
    "        last_cocluster_idx = len(ts_series) - 1\n",
    "        for j in range(len(ts_series) - 1, -1, -1):\n",
    "            if ts_series[j][1] == 1:\n",
    "                last_cocluster_idx = j\n",
    "                break\n",
    "        duration = max(1, last_cocluster_idx - formation_idx + 1) if formation_idx is not None else 1\n",
    "        dissolution_ts = all_timestamps[min(last_cocluster_idx + 1, len(all_timestamps) - 1)]\n",
    "        \n",
    "        dissolution_events.append({\n",
    "            'Ticker_1': pair[0],\n",
    "            'Ticker_2': pair[1],\n",
    "            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "            'Dissolution_Time': dissolution_ts,\n",
    "            'Duration_Hours': duration\n",
    "        })\n",
    "        \n",
    "        cluster_durations.append({\n",
    "            'Ticker_1': pair[0],\n",
    "            'Ticker_2': pair[1],\n",
    "            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "            'Formation_Time': formation_ts,\n",
    "            'Dissolution_Time': dissolution_ts,\n",
    "            'Duration_Hours': duration\n",
    "        })\n",
    "\n",
    "df_formations = pd.DataFrame(formation_events)\n",
    "df_dissolutions = pd.DataFrame(dissolution_events)\n",
    "df_durations = pd.DataFrame(cluster_durations)\n",
    "\n",
    "print(f\"\\nFormation events detected: {len(df_formations)}\")\n",
    "print(f\"Dissolution events detected: {len(df_dissolutions)}\")\n",
    "print(f\"Complete cluster episodes (formation + dissolution): {len(df_durations)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Duration Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if len(df_durations) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CLUSTER DURATION ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nDuration Statistics (hours):\")\n",
    "    print(f\"  Mean:   {df_durations['Duration_Hours'].mean():.1f}\")\n",
    "    print(f\"  Median: {df_durations['Duration_Hours'].median():.1f}\")\n",
    "    print(f\"  Min:    {df_durations['Duration_Hours'].min():.0f}\")\n",
    "    print(f\"  Max:    {df_durations['Duration_Hours'].max():.0f}\")\n",
    "    print(f\"  Std:    {df_durations['Duration_Hours'].std():.1f}\")\n",
    "    \n",
    "    # Duration buckets\n",
    "    short_lived = len(df_durations[df_durations['Duration_Hours'] <= 10])\n",
    "    medium_lived = len(df_durations[(df_durations['Duration_Hours'] > 10) & (df_durations['Duration_Hours'] <= 50)])\n",
    "    long_lived = len(df_durations[df_durations['Duration_Hours'] > 50])\n",
    "    \n",
    "    print(f\"\\nDuration Buckets:\")\n",
    "    print(f\"  Short-lived (<=10h):  {short_lived} ({short_lived/len(df_durations):.1%})\")\n",
    "    print(f\"  Medium (10-50h):      {medium_lived} ({medium_lived/len(df_durations):.1%})\")\n",
    "    print(f\"  Long-lived (>50h):    {long_lived} ({long_lived/len(df_durations):.1%})\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Classify pairs\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PAIR CLASSIFICATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Transient pairs: multiple formation events with short durations\n",
    "pair_formation_counts = df_formations.groupby('Pair').size().reset_index(name='Formation_Count')\n",
    "pair_avg_duration = df_durations.groupby('Pair')['Duration_Hours'].mean().reset_index(name='Avg_Duration')\n",
    "\n",
    "pair_classification = pair_formation_counts.merge(pair_avg_duration, on='Pair', how='left')\n",
    "pair_classification = pair_classification.merge(\n",
    "    df_pair_stability[['Pair', 'Co_Cluster_Frequency']], on='Pair', how='left'\n",
    ")\n",
    "\n",
    "# Classify\n",
    "pair_classification['Category'] = 'unknown'\n",
    "\n",
    "# Transient: multiple short-lived formations (the target use case)\n",
    "transient_mask = (pair_classification['Formation_Count'] >= 3) & (pair_classification['Avg_Duration'] <= 30)\n",
    "pair_classification.loc[transient_mask, 'Category'] = 'transient'\n",
    "\n",
    "# Stable: high co-clustering frequency OR very long average duration\n",
    "stable_mask = ((pair_classification['Co_Cluster_Frequency'] > 0.25) | (pair_classification['Avg_Duration'] > 100)) & (pair_classification['Category'] == 'unknown')\n",
    "pair_classification.loc[stable_mask, 'Category'] = 'stable_candidate'\n",
    "\n",
    "# Sporadic: few formation events\n",
    "sporadic_mask = (pair_classification['Formation_Count'] <= 2) & (pair_classification['Category'] == 'unknown')\n",
    "pair_classification.loc[sporadic_mask, 'Category'] = 'sporadic'\n",
    "\n",
    "for cat in ['transient', 'stable_candidate', 'sporadic', 'unknown']:\n",
    "    count = len(pair_classification[pair_classification['Category'] == cat])\n",
    "    print(f\"  {cat:20s}: {count} pairs\")\n",
    "\n",
    "# Show top transient pairs\n",
    "transient_pairs_classified = pair_classification[pair_classification['Category'] == 'transient'].sort_values('Formation_Count', ascending=False)\n",
    "if len(transient_pairs_classified) > 0:\n",
    "    print(f\"\\nTop 15 Transient Pairs (target for prediction):\")\n",
    "    print(transient_pairs_classified[['Pair', 'Formation_Count', 'Avg_Duration', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "# Show stable/cointegration candidates\n",
    "stable_candidates = pair_classification[pair_classification['Category'] == 'stable_candidate'].sort_values('Co_Cluster_Frequency', ascending=False)\n",
    "if len(stable_candidates) > 0:\n",
    "    print(f\"\\nStable/Cointegration Candidates (consistently co-clustered):\")\n",
    "    print(stable_candidates[['Pair', 'Formation_Count', 'Avg_Duration', 'Co_Cluster_Frequency']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FORMATION/DISSOLUTION ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nData structures created:\")\n",
    "print(f\"  - df_formations: {len(df_formations)} formation events\")\n",
    "print(f\"  - df_dissolutions: {len(df_dissolutions)} dissolution events\")\n",
    "print(f\"  - df_durations: {len(df_durations)} complete cluster episodes\")\n",
    "print(f\"  - pair_classification: {len(pair_classification)} pairs classified\")\n",
    "print(f\"\\nUse df_formations for validation (these are the events to test)\")\n",
    "print(f\"Use transient pairs for prediction modeling\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 5: Filter for actionable durations\n",
    "# ============================================================================\n",
    "# Short-lived episodes (<5h) dissolve before a trade can be entered and\n",
    "# validated.  Keep only episodes with Duration_Hours >= MIN_EPISODE_HOURS.\n",
    "\n",
    "MIN_EPISODE_HOURS = 5\n",
    "\n",
    "if len(df_durations) > 0:\n",
    "    df_formations_actionable = df_formations.merge(\n",
    "        df_durations[['Pair', 'Formation_Time', 'Duration_Hours']],\n",
    "        on=['Pair', 'Formation_Time'], how='inner'\n",
    "    ).query('Duration_Hours >= @MIN_EPISODE_HOURS')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ACTIONABLE FORMATION EVENTS (FIX 5)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  All formation events:        {len(df_formations)}\")\n",
    "    print(f\"  With duration >= {MIN_EPISODE_HOURS}h:         {len(df_formations_actionable)}\")\n",
    "    \n",
    "    for bucket_h in [5, 10, 20]:\n",
    "        n = len(df_formations_actionable[df_formations_actionable['Duration_Hours'] >= bucket_h])\n",
    "        print(f\"  With duration >= {bucket_h}h:        {n}\")\n",
    "else:\n",
    "    df_formations_actionable = df_formations.copy()\n",
    "    print(\"WARNING: No duration data; using all formations as actionable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORRECTNESS CHECKS: Feature-Shuffle Permutation Test, OOS Split, Sensitivity\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# 1. FEATURE-SHUFFLE PERMUTATION TEST (FIX 1)\n",
    "# ============================================================================\n",
    "# The old test shuffled cluster *labels*, which preserves cluster sizes and\n",
    "# therefore preserves co-clustering rates by construction (tautological).\n",
    "#\n",
    "# The correct approach: shuffle the *feature vectors* across tickers at each\n",
    "# timestamp, then re-run the full StandardScaler -> PCA -> OPTICS pipeline.\n",
    "# This breaks the ticker-feature mapping while preserving the joint\n",
    "# distribution of features, providing a proper null hypothesis.\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 1: Feature-Shuffle Permutation Test (FIX 1)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "optics_params = dict(min_samples=3, metric='euclidean', xi=0.05, min_cluster_size=3)\n",
    "features_to_cluster = [\n",
    "    'Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short',\n",
    "    'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift',\n",
    "    'Beta_Sector_Regime_Shift'\n",
    "]\n",
    "\n",
    "perm_result = feature_shuffle_permutation_test(\n",
    "    ts_df=ts_df,\n",
    "    features_to_cluster=features_to_cluster,\n",
    "    optics_params=optics_params,\n",
    "    pair_co_cluster_freq=pair_co_cluster_freq,\n",
    "    total_valid_windows=cluster_history['Datetime'].nunique(),\n",
    "    n_permutations=30,\n",
    "    n_sample_timestamps=80,\n",
    ")\n",
    "\n",
    "print(f\"\\nFraction of pairs significant at p<0.05: {perm_result['fraction_significant']:.1%}\")\n",
    "\n",
    "# Show top pairs by Z-score\n",
    "top_z = sorted(perm_result['pair_zscores'].items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(f\"\\nTop 15 pairs by permutation Z-score:\")\n",
    "for pair, z in top_z:\n",
    "    print(f\"  {pair[0]:6s}-{pair[1]:6s}  Z = {z:.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. OUT-OF-SAMPLE SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 2: Out-of-Sample Split\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_dates = sorted(cluster_history['Datetime'].unique())\n",
    "split_point = int(len(all_dates) * 0.67)\n",
    "split_timestamp = all_dates[split_point]\n",
    "\n",
    "train_history = cluster_history[cluster_history['Datetime'] <= split_timestamp]\n",
    "test_history = cluster_history[cluster_history['Datetime'] > split_timestamp]\n",
    "\n",
    "print(f\"\\nSplit point: {split_timestamp}\")\n",
    "print(f\"Training period: {all_dates[0]} to {split_timestamp} ({len(train_history)} rows)\")\n",
    "print(f\"Test period: {split_timestamp} to {all_dates[-1]} ({len(test_history)} rows)\")\n",
    "\n",
    "def calc_pair_freq(history_df):\n",
    "    freq = {}\n",
    "    total = history_df['Datetime'].nunique()\n",
    "    for ts in history_df['Datetime'].unique():\n",
    "        snap = history_df[history_df['Datetime'] == ts]\n",
    "        for cid in snap['Cluster_ID'].unique():\n",
    "            if cid == -1:\n",
    "                continue\n",
    "            members = sorted(snap[snap['Cluster_ID'] == cid]['Ticker'].tolist())\n",
    "            for s1, s2 in itertools.combinations(members, 2):\n",
    "                freq[(s1, s2)] = freq.get((s1, s2), 0) + 1\n",
    "    return {k: v / total for k, v in freq.items()}\n",
    "\n",
    "train_freq = calc_pair_freq(train_history)\n",
    "test_freq = calc_pair_freq(test_history)\n",
    "\n",
    "common_pairs = set(train_freq.keys()) & set(test_freq.keys())\n",
    "if len(common_pairs) > 0:\n",
    "    train_vals = [train_freq[p] for p in common_pairs]\n",
    "    test_vals = [test_freq[p] for p in common_pairs]\n",
    "    from scipy.stats import pearsonr\n",
    "    oos_corr, oos_pval = pearsonr(train_vals, test_vals)\n",
    "    \n",
    "    print(f\"\\nPairs observed in both periods: {len(common_pairs)}\")\n",
    "    print(f\"Correlation of co-clustering frequency (train vs test): {oos_corr:.3f} (p={oos_pval:.4f})\")\n",
    "    \n",
    "    if oos_corr > 0.5:\n",
    "        print(\"RESULT: Good out-of-sample stability\")\n",
    "    elif oos_corr > 0.2:\n",
    "        print(\"RESULT: Moderate out-of-sample stability\")\n",
    "    else:\n",
    "        print(\"WARNING: Poor out-of-sample stability\")\n",
    "else:\n",
    "    print(\"WARNING: No common pairs between train and test periods\")\n",
    "\n",
    "oos_split_timestamp = split_timestamp\n",
    "\n",
    "# ============================================================================\n",
    "# 3. OPTICS PARAMETER SENSITIVITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 3: OPTICS Parameter Sensitivity\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "param_configs = [\n",
    "    {'min_samples': 2, 'xi': 0.05, 'min_cluster_size': 2},\n",
    "    {'min_samples': 3, 'xi': 0.05, 'min_cluster_size': 3},\n",
    "    {'min_samples': 3, 'xi': 0.03, 'min_cluster_size': 3},\n",
    "    {'min_samples': 5, 'xi': 0.05, 'min_cluster_size': 5},\n",
    "    {'min_samples': 3, 'xi': 0.10, 'min_cluster_size': 3},\n",
    "]\n",
    "\n",
    "sample_timestamps_sens = valid_timestamps[::max(1, len(valid_timestamps) // 50)][:50]\n",
    "print(f\"Testing {len(param_configs)} configs on {len(sample_timestamps_sens)} timestamps...\\n\")\n",
    "\n",
    "sensitivity_results = []\n",
    "for config in param_configs:\n",
    "    config_clusters = []\n",
    "    config_noise = []\n",
    "    for ts in sample_timestamps_sens:\n",
    "        try:\n",
    "            snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "            if len(snapshot) < 5:\n",
    "                continue\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(snapshot)\n",
    "            pca = PCA(n_components=0.90)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "            optics = OPTICS(min_samples=config['min_samples'], metric='euclidean',\n",
    "                            xi=config['xi'], min_cluster_size=config['min_cluster_size'])\n",
    "            optics.fit(X_pca)\n",
    "            n_clusters = len(set(optics.labels_)) - (1 if -1 in optics.labels_ else 0)\n",
    "            noise_pct = (optics.labels_ == -1).sum() / len(optics.labels_)\n",
    "            config_clusters.append(n_clusters)\n",
    "            config_noise.append(noise_pct)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if config_clusters:\n",
    "        sensitivity_results.append({\n",
    "            'min_samples': config['min_samples'],\n",
    "            'xi': config['xi'],\n",
    "            'min_cluster_size': config['min_cluster_size'],\n",
    "            'avg_clusters': np.mean(config_clusters),\n",
    "            'avg_noise_pct': np.mean(config_noise),\n",
    "            'std_clusters': np.std(config_clusters)\n",
    "        })\n",
    "\n",
    "df_sensitivity = pd.DataFrame(sensitivity_results)\n",
    "print(\"Parameter Sensitivity Results:\")\n",
    "print(df_sensitivity.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL CORRECTNESS CHECKS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE ARTIFACTS FOR SIGNALS NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "save_items = {\n",
    "    'ts_df': ts_df,\n",
    "    'df_formations': df_formations,\n",
    "    'df_formations_actionable': df_formations_actionable,\n",
    "    'pair_classification': pair_classification,\n",
    "    'cluster_history': cluster_history,\n",
    "    'df_durations': df_durations,\n",
    "    'df_pair_stability': df_pair_stability,\n",
    "    'oos_split_timestamp': oos_split_timestamp,\n",
    "    'pair_co_cluster_freq': pair_co_cluster_freq,\n",
    "}\n",
    "\n",
    "for name, obj in save_items.items():\n",
    "    path = os.path.join(data_dir, f'{name}.pkl')\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    size = len(obj) if hasattr(obj, '__len__') else 1\n",
    "    print(f'Saved {name} -> {path} ({size} items)')\n",
    "\n",
    "print(f'\\nAll artifacts saved to {data_dir}/')\n",
    "print('optics-signals.ipynb can now load these files.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
