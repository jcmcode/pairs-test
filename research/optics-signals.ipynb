{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTICS Signals: Transient & Stable Pair Strategies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & LOAD ARTIFACTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, norm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath('__file__')), '..'))\n",
    "from validation.pair_validation import (\n",
    "    compute_hedge_ratio, hedge_ratio_drift, spread_cv_normalized,\n",
    "    half_life, zscore_signals, simulate_spread_pnl,\n",
    ")\n",
    "\n",
    "# Load artifacts from Notebook 1\n",
    "data_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data')\n",
    "\n",
    "artifact_names = [\n",
    "    'ts_df', 'df_formations', 'df_formations_actionable',\n",
    "    'pair_classification', 'cluster_history', 'df_durations',\n",
    "    'df_pair_stability', 'oos_split_timestamp', 'pair_co_cluster_freq',\n",
    "]\n",
    "\n",
    "artifacts = {}\n",
    "for name in artifact_names:\n",
    "    path = os.path.join(data_dir, f'{name}.pkl')\n",
    "    with open(path, 'rb') as f:\n",
    "        artifacts[name] = pickle.load(f)\n",
    "    print(f'Loaded {name} ({type(artifacts[name]).__name__})')\n",
    "\n",
    "# Unpack into local variables\n",
    "ts_df = artifacts['ts_df']\n",
    "df_formations = artifacts['df_formations']\n",
    "df_formations_actionable = artifacts['df_formations_actionable']\n",
    "pair_classification = artifacts['pair_classification']\n",
    "cluster_history = artifacts['cluster_history']\n",
    "df_durations = artifacts['df_durations']\n",
    "df_pair_stability = artifacts['df_pair_stability']\n",
    "oos_split_timestamp = artifacts['oos_split_timestamp']\n",
    "pair_co_cluster_freq = artifacts['pair_co_cluster_freq']\n",
    "\n",
    "print(f\"\\nLoaded all artifacts successfully.\")\n",
    "print(f\"ts_df: {len(ts_df)} rows, {ts_df.index.get_level_values('Ticker').nunique()} tickers\")\n",
    "print(f\"Actionable formations: {len(df_formations_actionable)}\")\n",
    "print(f\"Pair classifications: {len(pair_classification)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_price_window(ticker, timestamp, window_obs, ts_df, direction='forward'):\n",
    "    \"\"\"\n",
    "    Extract price data for a ticker over a window of N observations.\n",
    "    Uses the MultiIndex (Datetime, Ticker) structure of ts_df.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ticker_data = ts_df.xs(ticker, level='Ticker').sort_index()\n",
    "        if timestamp not in ticker_data.index:\n",
    "            idx_pos = ticker_data.index.searchsorted(timestamp)\n",
    "            if idx_pos >= len(ticker_data.index):\n",
    "                return pd.Series(dtype=float)\n",
    "            timestamp = ticker_data.index[idx_pos]\n",
    "        loc = ticker_data.index.get_loc(timestamp)\n",
    "        if direction == 'forward':\n",
    "            end_loc = min(loc + window_obs, len(ticker_data))\n",
    "            return ticker_data.iloc[loc:end_loc]['Price']\n",
    "        else:\n",
    "            start_loc = max(0, loc - window_obs + 1)\n",
    "            return ticker_data.iloc[start_loc:loc + 1]['Price']\n",
    "    except Exception:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "\n",
    "def get_daily_prices(ticker, ts_df):\n",
    "    \"\"\"Resample hourly prices to daily (last observation per day).\"\"\"\n",
    "    try:\n",
    "        ticker_data = ts_df.xs(ticker, level='Ticker').sort_index()['Price']\n",
    "        return ticker_data.resample('1D').last().dropna()\n",
    "    except Exception:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "\n",
    "def validate_transient_event(ticker_a, ticker_b, timestamp, ts_df, config):\n",
    "    \"\"\"\n",
    "    Validate a single transient pair event using three-window approach:\n",
    "      - Execution lag: skip first EXECUTION_LAG obs after formation\n",
    "      - Calibration window: estimate hedge ratio\n",
    "      - Exploitation window: test spread metrics + generate signals + simulate P&L\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict with keys CALIBRATION, EXPLOITATION, EXECUTION_LAG,\n",
    "             CORR_THRESHOLD, CV_THRESHOLD, HL_MAX, HEDGE_DRIFT_MAX, ENTRY_Z, EXIT_Z\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with all metrics + pass/fail, or None if insufficient data.\n",
    "    \"\"\"\n",
    "    lag = config['EXECUTION_LAG']\n",
    "    cal_len = config['CALIBRATION']\n",
    "    exploit_len = config['EXPLOITATION']\n",
    "    total_need = lag + cal_len + exploit_len\n",
    "\n",
    "    prices_a = get_price_window(ticker_a, timestamp, total_need, ts_df, direction='forward')\n",
    "    prices_b = get_price_window(ticker_b, timestamp, total_need, ts_df, direction='forward')\n",
    "\n",
    "    if len(prices_a) < total_need or len(prices_b) < total_need:\n",
    "        return None\n",
    "\n",
    "    common_idx = prices_a.index.intersection(prices_b.index)\n",
    "    pa = prices_a.loc[common_idx]\n",
    "    pb = prices_b.loc[common_idx]\n",
    "    if len(pa) < total_need:\n",
    "        return None\n",
    "\n",
    "    # Split into windows\n",
    "    pa_cal = pa.iloc[lag:lag + cal_len]\n",
    "    pb_cal = pb.iloc[lag:lag + cal_len]\n",
    "    pa_exploit = pa.iloc[lag + cal_len:lag + cal_len + exploit_len]\n",
    "    pb_exploit = pb.iloc[lag + cal_len:lag + cal_len + exploit_len]\n",
    "\n",
    "    if len(pa_cal) < 5 or len(pa_exploit) < 10:\n",
    "        return None\n",
    "\n",
    "    # Calibration: estimate hedge ratio\n",
    "    beta_cal, intercept_cal, r_sq_cal = compute_hedge_ratio(pa_cal, pb_cal, method='ols')\n",
    "    if np.isnan(beta_cal):\n",
    "        return None\n",
    "\n",
    "    # Exploitation: compute metrics\n",
    "    spread_exploit = pa_exploit - beta_cal * pb_exploit\n",
    "    cv = spread_cv_normalized(spread_exploit, pa_exploit, pb_exploit)\n",
    "    hl = half_life(spread_exploit)\n",
    "\n",
    "    # Returns correlation on exploitation window\n",
    "    ret_a = pa_exploit.pct_change().dropna()\n",
    "    ret_b = pb_exploit.pct_change().dropna()\n",
    "    common_ret = ret_a.index.intersection(ret_b.index)\n",
    "    if len(common_ret) < 10:\n",
    "        corr = np.nan\n",
    "    else:\n",
    "        corr, _ = pearsonr(ret_a.loc[common_ret], ret_b.loc[common_ret])\n",
    "\n",
    "    # Hedge ratio drift (FIX 6)\n",
    "    beta_exploit_val, _, _ = compute_hedge_ratio(pa_exploit, pb_exploit, method='ols')\n",
    "    drift = hedge_ratio_drift(beta_cal, beta_exploit_val)\n",
    "\n",
    "    # Z-score signals on exploitation spread\n",
    "    lookback = min(20, len(spread_exploit) // 2)\n",
    "    if lookback < 5:\n",
    "        lookback = 5\n",
    "    signals = zscore_signals(spread_exploit, lookback=lookback,\n",
    "                             entry_z=config['ENTRY_Z'], exit_z=config['EXIT_Z'])\n",
    "\n",
    "    # Simulate P&L\n",
    "    pnl_result = simulate_spread_pnl(spread_exploit, signals, cost_per_trade=0.0)\n",
    "\n",
    "    has_signal = signals['long_entry'].any() or signals['short_entry'].any()\n",
    "\n",
    "    # Pass/fail\n",
    "    passed = (\n",
    "        (not np.isnan(corr)) and corr > config['CORR_THRESHOLD']\n",
    "        and (not np.isnan(cv)) and cv < config['CV_THRESHOLD']\n",
    "        and (not np.isnan(hl)) and hl < config['HL_MAX'] and hl > 0\n",
    "        and (not np.isnan(drift)) and drift < config['HEDGE_DRIFT_MAX']\n",
    "        and has_signal\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'ticker_a': ticker_a,\n",
    "        'ticker_b': ticker_b,\n",
    "        'timestamp': timestamp,\n",
    "        'beta_cal': beta_cal,\n",
    "        'r_squared_cal': r_sq_cal,\n",
    "        'correlation': corr,\n",
    "        'spread_cv': cv,\n",
    "        'half_life': hl,\n",
    "        'hedge_drift': drift,\n",
    "        'has_signal': has_signal,\n",
    "        'n_trades': pnl_result['n_trades'],\n",
    "        'total_pnl': pnl_result['total_pnl'],\n",
    "        'win_rate': pnl_result['win_rate'],\n",
    "        'sharpe': pnl_result['sharpe'],\n",
    "        'max_drawdown': pnl_result['max_drawdown'],\n",
    "        'passed': passed,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transient Pair Strategy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# TRANSIENT PAIR VALIDATION (FIX 2, 3, 5, 6)\n",
    "# ============================================================================\n",
    "# FIX 2: Separate calibration/exploitation windows (no circular validation)\n",
    "# FIX 3: Tighter, transient-appropriate thresholds (drop cointegration)\n",
    "# FIX 5: Only use actionable formations (>= 5h duration)\n",
    "# FIX 6: Check hedge ratio drift between calibration and exploitation\n",
    "\n",
    "TRANSIENT_CONFIG = {\n",
    "    'CALIBRATION': 10,        # hours for hedge ratio estimation\n",
    "    'EXPLOITATION': 40,       # hours for out-of-sample testing\n",
    "    'EXECUTION_LAG': 2,       # hours delay before entering (FIX 5)\n",
    "    'CORR_THRESHOLD': 0.70,   # returns correlation (FIX 3: tighter)\n",
    "    'CV_THRESHOLD': 0.03,     # spread CV normalized (FIX 3: tighter)\n",
    "    'HL_MAX': 8,              # max half-life in hours (FIX 3: transient)\n",
    "    'HEDGE_DRIFT_MAX': 0.20,  # max hedge ratio drift (FIX 6)\n",
    "    'ENTRY_Z': 2.0,\n",
    "    'EXIT_Z': 0.5,\n",
    "}\n",
    "\n",
    "MAX_PAIRS = 50\n",
    "MAX_EVENTS_PER_PAIR = 20\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"TRANSIENT PAIR VALIDATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nConfig: {TRANSIENT_CONFIG}\")\n",
    "print(f\"Max pairs: {MAX_PAIRS}, max events/pair: {MAX_EVENTS_PER_PAIR}\")\n",
    "\n",
    "# Select transient pairs from classification\n",
    "transient_classified = pair_classification[pair_classification['Category'] == 'transient']\n",
    "transient_pair_names = set(transient_classified['Pair'].values)\n",
    "transient_pairs = df_pair_stability[\n",
    "    df_pair_stability['Pair'].isin(transient_pair_names)\n",
    "].head(MAX_PAIRS)\n",
    "\n",
    "print(f\"Testing {len(transient_pairs)} transient pairs\")\n",
    "\n",
    "# Validation loop on actionable formations\n",
    "transient_results = []\n",
    "for _, pair_row in transient_pairs.iterrows():\n",
    "    ticker_a = pair_row['Ticker_1']\n",
    "    ticker_b = pair_row['Ticker_2']\n",
    "    pair_name = f\"{ticker_a}-{ticker_b}\"\n",
    "\n",
    "    pair_events = df_formations_actionable[\n",
    "        df_formations_actionable['Pair'] == pair_name\n",
    "    ].head(MAX_EVENTS_PER_PAIR)\n",
    "\n",
    "    for _, event in pair_events.iterrows():\n",
    "        result = validate_transient_event(\n",
    "            ticker_a, ticker_b, event['Formation_Time'],\n",
    "            ts_df, TRANSIENT_CONFIG\n",
    "        )\n",
    "        if result is not None:\n",
    "            transient_results.append(result)\n",
    "\n",
    "df_transient = pd.DataFrame(transient_results)\n",
    "\n",
    "print(f\"\\nResults: {len(df_transient)} events tested\")\n",
    "if len(df_transient) > 0:\n",
    "    n_passed = df_transient['passed'].sum()\n",
    "    print(f\"Passed: {n_passed} ({n_passed/len(df_transient):.1%})\")\n",
    "    print(f\"Avg P&L (passed): {df_transient[df_transient['passed']]['total_pnl'].mean():.4f}\")\n",
    "    print(f\"Avg trades (passed): {df_transient[df_transient['passed']]['n_trades'].mean():.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# RANDOM-PAIR BASELINE (FIX 2)\n",
    "# ============================================================================\n",
    "# Test the same validation on NON-co-clustered pairs at the same timestamps.\n",
    "# If clustered pairs don't outperform random pairs, the clustering adds\n",
    "# no value to pair selection.\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"RANDOM-PAIR BASELINE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Select pairs that rarely co-cluster (frequency < 0.03)\n",
    "rare_pairs = df_pair_stability[\n",
    "    df_pair_stability['Co_Cluster_Frequency'] < 0.03\n",
    "]\n",
    "\n",
    "if len(rare_pairs) == 0:\n",
    "    print(\"WARNING: No rare co-clustering pairs found for baseline.\")\n",
    "    df_random = pd.DataFrame()\n",
    "else:\n",
    "    # Sample up to MAX_PAIRS random pairs\n",
    "    random_pairs = rare_pairs.sample(n=min(MAX_PAIRS, len(rare_pairs)), random_state=42)\n",
    "\n",
    "    # Use same formation timestamps from the transient analysis\n",
    "    formation_timestamps = df_formations_actionable['Formation_Time'].unique()\n",
    "    rng = np.random.default_rng(42)\n",
    "    sample_timestamps = rng.choice(\n",
    "        formation_timestamps,\n",
    "        size=min(100, len(formation_timestamps)),\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    random_results = []\n",
    "    for _, pair_row in random_pairs.iterrows():\n",
    "        ticker_a = pair_row['Ticker_1']\n",
    "        ticker_b = pair_row['Ticker_2']\n",
    "\n",
    "        for ts in sample_timestamps[:MAX_EVENTS_PER_PAIR]:\n",
    "            result = validate_transient_event(\n",
    "                ticker_a, ticker_b, ts,\n",
    "                ts_df, TRANSIENT_CONFIG\n",
    "            )\n",
    "            if result is not None:\n",
    "                random_results.append(result)\n",
    "\n",
    "    df_random = pd.DataFrame(random_results)\n",
    "\n",
    "    print(f\"\\nRandom baseline: {len(df_random)} events tested\")\n",
    "    if len(df_random) > 0:\n",
    "        n_passed_rand = df_random['passed'].sum()\n",
    "        print(f\"Passed: {n_passed_rand} ({n_passed_rand/len(df_random):.1%})\")\n",
    "        print(f\"Avg P&L (passed): {df_random[df_random['passed']]['total_pnl'].mean():.4f}\" if n_passed_rand > 0 else \"No passes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# TRANSIENT RESULTS ANALYSIS: Clustered vs Random\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"CLUSTERED vs RANDOM COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if len(df_transient) > 0:\n",
    "    clust_pass_rate = df_transient['passed'].mean()\n",
    "    print(f\"\\nClustered pairs:\")\n",
    "    print(f\"  Events tested:  {len(df_transient)}\")\n",
    "    print(f\"  Pass rate:      {clust_pass_rate:.1%}\")\n",
    "    print(f\"  Avg correlation (all): {df_transient['correlation'].mean():.3f}\")\n",
    "    print(f\"  Avg spread CV (all):   {df_transient['spread_cv'].mean():.4f}\")\n",
    "    print(f\"  Avg half-life (all):   {df_transient['half_life'].replace([np.inf], np.nan).mean():.1f}h\")\n",
    "    print(f\"  Avg hedge drift (all): {df_transient['hedge_drift'].mean():.3f}\")\n",
    "\n",
    "    passed_trans = df_transient[df_transient['passed']]\n",
    "    if len(passed_trans) > 0:\n",
    "        print(f\"\\n  Passed events:\")\n",
    "        print(f\"    Avg P&L:       {passed_trans['total_pnl'].mean():.4f}\")\n",
    "        print(f\"    Avg win rate:  {passed_trans['win_rate'].mean():.1%}\")\n",
    "        print(f\"    Avg trades:    {passed_trans['n_trades'].mean():.1f}\")\n",
    "\n",
    "        # Top pairs\n",
    "        pair_counts = passed_trans.groupby(['ticker_a', 'ticker_b']).agg(\n",
    "            n_passed=('passed', 'sum'),\n",
    "            avg_pnl=('total_pnl', 'mean'),\n",
    "            avg_cv=('spread_cv', 'mean'),\n",
    "        ).sort_values('n_passed', ascending=False).head(10)\n",
    "        print(f\"\\n  Top 10 pairs by pass count:\")\n",
    "        print(pair_counts.to_string())\n",
    "\n",
    "if len(df_random) > 0:\n",
    "    rand_pass_rate = df_random['passed'].mean()\n",
    "    print(f\"\\nRandom (non-clustered) pairs:\")\n",
    "    print(f\"  Events tested:  {len(df_random)}\")\n",
    "    print(f\"  Pass rate:      {rand_pass_rate:.1%}\")\n",
    "    print(f\"  Avg correlation: {df_random['correlation'].mean():.3f}\")\n",
    "    print(f\"  Avg spread CV:   {df_random['spread_cv'].mean():.4f}\")\n",
    "\n",
    "    if len(df_transient) > 0:\n",
    "        lift = clust_pass_rate / rand_pass_rate if rand_pass_rate > 0 else float('inf')\n",
    "        print(f\"\\n  Clustered lift over random: {lift:.1f}x\")\n",
    "\n",
    "        # P&L comparison\n",
    "        clust_pnl = df_transient[df_transient['passed']]['total_pnl'].values\n",
    "        rand_pnl = df_random[df_random['passed']]['total_pnl'].values\n",
    "        if len(clust_pnl) > 0 and len(rand_pnl) > 0:\n",
    "            print(f\"  Avg P&L clustered: {clust_pnl.mean():.4f}\")\n",
    "            print(f\"  Avg P&L random:    {rand_pnl.mean():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# DURATION PREDICTION: Will episode last >= 10h? (FIX 5)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"DURATION PREDICTION MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "DURATION_TARGET_HOURS = 10\n",
    "\n",
    "if len(df_durations) < 50:\n",
    "    print(\"Insufficient duration data for modeling.\")\n",
    "else:\n",
    "    # Build feature matrix from formation-time features\n",
    "    duration_features = []\n",
    "    for _, row in df_durations.iterrows():\n",
    "        ticker_a, ticker_b = row['Ticker_1'], row['Ticker_2']\n",
    "        ts = row['Formation_Time']\n",
    "        target = 1 if row['Duration_Hours'] >= DURATION_TARGET_HOURS else 0\n",
    "\n",
    "        try:\n",
    "            feat_a = ts_df.xs(ts, level='Datetime').xs(ticker_a, level='Ticker')\n",
    "            feat_b = ts_df.xs(ts, level='Datetime').xs(ticker_b, level='Ticker')\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # Average features across the pair\n",
    "        feat_cols = [c for c in ts_df.columns if c not in ['Price', 'Returns']]\n",
    "        feat_avg = {}\n",
    "        for col in feat_cols:\n",
    "            if col in feat_a.index and col in feat_b.index:\n",
    "                feat_avg[col] = (feat_a[col] + feat_b[col]) / 2.0\n",
    "        feat_avg['target'] = target\n",
    "        feat_avg['formation_ts'] = ts\n",
    "        duration_features.append(feat_avg)\n",
    "\n",
    "    df_dur_model = pd.DataFrame(duration_features).dropna()\n",
    "\n",
    "    if len(df_dur_model) < 30:\n",
    "        print(f\"Only {len(df_dur_model)} samples with features; skipping model.\")\n",
    "    else:\n",
    "        # Sort by time for proper CV\n",
    "        df_dur_model = df_dur_model.sort_values('formation_ts').reset_index(drop=True)\n",
    "        feature_cols = [c for c in df_dur_model.columns if c not in ['target', 'formation_ts']]\n",
    "        X = df_dur_model[feature_cols].values\n",
    "        y = df_dur_model['target'].values\n",
    "\n",
    "        print(f\"Samples: {len(df_dur_model)}, Features: {len(feature_cols)}\")\n",
    "        print(f\"Positive rate (>= {DURATION_TARGET_HOURS}h): {y.mean():.1%}\")\n",
    "\n",
    "        # TimeSeriesSplit CV\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        aucs, precs, recs = [], [], []\n",
    "\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "                continue\n",
    "\n",
    "            model = LogisticRegression(max_iter=1000, C=0.1)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            aucs.append(roc_auc_score(y_test, y_prob))\n",
    "            precs.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "            recs.append(recall_score(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        if aucs:\n",
    "            print(f\"\\nTimeSeriesSplit CV Results (5 folds):\")\n",
    "            print(f\"  AUC:       {np.mean(aucs):.3f} +/- {np.std(aucs):.3f}\")\n",
    "            print(f\"  Precision: {np.mean(precs):.3f} +/- {np.std(precs):.3f}\")\n",
    "            print(f\"  Recall:    {np.mean(recs):.3f} +/- {np.std(recs):.3f}\")\n",
    "        else:\n",
    "            print(\"Could not compute CV metrics (insufficient class diversity).\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Pair Strategy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STABLE PAIR STRATEGY (FIX 3: Separate stable criteria)\n",
    "# ============================================================================\n",
    "# For stable_candidate pairs: daily data, cointegration, Hurst, Bollinger\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"STABLE PAIR VALIDATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "stable_candidates = pair_classification[\n",
    "    pair_classification['Category'] == 'stable_candidate'\n",
    "].sort_values('Co_Cluster_Frequency', ascending=False) if 'pair_classification' in dir() else pd.DataFrame()\n",
    "\n",
    "print(f\"Stable candidates: {len(stable_candidates)}\")\n",
    "\n",
    "stable_results = []\n",
    "\n",
    "for _, row in stable_candidates.iterrows():\n",
    "    pair_name = row['Pair']\n",
    "    tickers = pair_name.split('-')\n",
    "    if len(tickers) != 2:\n",
    "        continue\n",
    "    ticker_a, ticker_b = tickers\n",
    "\n",
    "    # Resample to daily\n",
    "    daily_a = get_daily_prices(ticker_a, ts_df)\n",
    "    daily_b = get_daily_prices(ticker_b, ts_df)\n",
    "\n",
    "    # Align\n",
    "    common_daily = daily_a.index.intersection(daily_b.index)\n",
    "    if len(common_daily) < 100:\n",
    "        continue\n",
    "    da = daily_a.loc[common_daily]\n",
    "    db = daily_b.loc[common_daily]\n",
    "\n",
    "    # Hedge ratio on daily\n",
    "    beta, intercept, r_sq = compute_hedge_ratio(da, db, method='ols')\n",
    "    if np.isnan(beta):\n",
    "        continue\n",
    "\n",
    "    spread = da - beta * db\n",
    "\n",
    "    # Engle-Granger cointegration\n",
    "    try:\n",
    "        _, coint_pval, _ = coint(da, db)\n",
    "    except Exception:\n",
    "        coint_pval = 1.0\n",
    "\n",
    "    # Half-life on daily\n",
    "    hl = half_life(spread)\n",
    "\n",
    "    # Hurst exponent (variance ratio method)\n",
    "    def hurst_variance_ratio(ts, max_lag=20):\n",
    "        ts = np.asarray(ts, dtype=float)\n",
    "        ts = ts[~np.isnan(ts)]\n",
    "        if len(ts) < max_lag * 2:\n",
    "            return np.nan\n",
    "        lags = range(2, max_lag + 1)\n",
    "        tau = []\n",
    "        for lag in lags:\n",
    "            diffs = ts[lag:] - ts[:-lag]\n",
    "            tau.append(np.sqrt(np.nanstd(diffs)))\n",
    "        if any(t == 0 for t in tau):\n",
    "            return np.nan\n",
    "        log_lags = np.log(list(lags))\n",
    "        log_tau = np.log(tau)\n",
    "        coef = np.polyfit(log_lags, log_tau, 1)\n",
    "        return coef[0]\n",
    "\n",
    "    hurst = hurst_variance_ratio(spread.values)\n",
    "\n",
    "    # Spread CV\n",
    "    cv = spread_cv_normalized(spread, da, db)\n",
    "\n",
    "    # Pass criteria for stable\n",
    "    passed = (\n",
    "        coint_pval < 0.05\n",
    "        and (not np.isnan(hl)) and 5 <= hl <= 60\n",
    "        and (not np.isnan(hurst)) and hurst < 0.5\n",
    "    )\n",
    "\n",
    "    # Bollinger signals on daily spread\n",
    "    lookback_daily = min(20, len(spread) // 3)\n",
    "    if lookback_daily >= 5:\n",
    "        signals = zscore_signals(spread, lookback=lookback_daily, entry_z=2.0, exit_z=0.5)\n",
    "        pnl_result = simulate_spread_pnl(spread, signals)\n",
    "    else:\n",
    "        pnl_result = {'total_pnl': 0, 'n_trades': 0, 'win_rate': np.nan, 'sharpe': np.nan, 'max_drawdown': 0}\n",
    "\n",
    "    stable_results.append({\n",
    "        'pair': pair_name,\n",
    "        'ticker_a': ticker_a,\n",
    "        'ticker_b': ticker_b,\n",
    "        'n_daily_obs': len(common_daily),\n",
    "        'beta': beta,\n",
    "        'r_squared': r_sq,\n",
    "        'coint_pval': coint_pval,\n",
    "        'half_life_days': hl,\n",
    "        'hurst': hurst,\n",
    "        'spread_cv': cv,\n",
    "        'passed': passed,\n",
    "        'total_pnl': pnl_result['total_pnl'],\n",
    "        'n_trades': pnl_result['n_trades'],\n",
    "        'win_rate': pnl_result['win_rate'],\n",
    "        'sharpe': pnl_result['sharpe'],\n",
    "    })\n",
    "\n",
    "df_stable = pd.DataFrame(stable_results)\n",
    "\n",
    "if len(df_stable) > 0:\n",
    "    n_passed = df_stable['passed'].sum()\n",
    "    print(f\"\\nResults: {len(df_stable)} pairs tested\")\n",
    "    print(f\"Passed (cointegrated + mean-reverting + Hurst<0.5): {n_passed}\")\n",
    "\n",
    "    if n_passed > 0:\n",
    "        print(f\"\\nPassing stable pairs:\")\n",
    "        print(df_stable[df_stable['passed']][\n",
    "            ['pair', 'coint_pval', 'half_life_days', 'hurst', 'total_pnl', 'n_trades']\n",
    "        ].to_string(index=False))\n",
    "else:\n",
    "    print(\"No stable candidates found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STABLE PAIR RESULTS TABLE & CUMULATIVE P&L\n",
    "# ============================================================================\n",
    "\n",
    "if len(df_stable) > 0 and df_stable['passed'].sum() > 0:\n",
    "    passed_stable = df_stable[df_stable['passed']].sort_values('total_pnl', ascending=False)\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"STABLE PAIR DETAILED RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(passed_stable[[\n",
    "        'pair', 'n_daily_obs', 'beta', 'r_squared', 'coint_pval',\n",
    "        'half_life_days', 'hurst', 'spread_cv', 'total_pnl', 'win_rate', 'sharpe'\n",
    "    ]].to_string(index=False))\n",
    "\n",
    "    print(f\"\\nCumulative P&L across all stable pairs: {passed_stable['total_pnl'].sum():.4f}\")\n",
    "    print(f\"Average Sharpe: {passed_stable['sharpe'].dropna().mean():.2f}\")\n",
    "else:\n",
    "    print(\"No stable pairs passed all criteria.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# TRANSIENT STRATEGY VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "if len(df_transient) == 0:\n",
    "    print(\"No transient results to visualize.\")\n",
    "else:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('TRANSIENT STRATEGY ANALYSIS', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Pass rates: clustered vs random\n",
    "    ax = axes[0, 0]\n",
    "    labels = ['Clustered']\n",
    "    rates = [df_transient['passed'].mean() * 100]\n",
    "    colors = ['#2ecc71']\n",
    "    if len(df_random) > 0:\n",
    "        labels.append('Random')\n",
    "        rates.append(df_random['passed'].mean() * 100)\n",
    "        colors.append('#e74c3c')\n",
    "    bars = ax.bar(labels, rates, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., h + 1, f'{h:.1f}%',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    ax.set_ylabel('Pass Rate (%)')\n",
    "    ax.set_title('Pass Rate: Clustered vs Random')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 2. Correlation vs Spread CV scatter\n",
    "    ax = axes[0, 1]\n",
    "    scatter_data = df_transient.dropna(subset=['correlation', 'spread_cv'])\n",
    "    scatter_data = scatter_data[scatter_data['spread_cv'] < 0.2]\n",
    "    if len(scatter_data) > 0:\n",
    "        colors_scatter = scatter_data['passed'].map({True: '#2ecc71', False: '#e74c3c'})\n",
    "        ax.scatter(scatter_data['correlation'], scatter_data['spread_cv'],\n",
    "                   c=colors_scatter, alpha=0.6, s=40, edgecolors='black', linewidth=0.5)\n",
    "        ax.axhline(y=TRANSIENT_CONFIG['CV_THRESHOLD'], color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        ax.axvline(x=TRANSIENT_CONFIG['CORR_THRESHOLD'], color='blue', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    ax.set_xlabel('Correlation')\n",
    "    ax.set_ylabel('Spread CV')\n",
    "    ax.set_title('Correlation vs Spread CV')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    # 3. P&L distribution (passed events)\n",
    "    ax = axes[0, 2]\n",
    "    passed_pnl = df_transient[df_transient['passed']]['total_pnl'].dropna()\n",
    "    if len(passed_pnl) > 0:\n",
    "        ax.hist(passed_pnl, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax.axvline(x=passed_pnl.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {passed_pnl.mean():.4f}')\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('Total P&L')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('P&L Distribution (Passed Events)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 4. Spread CV distribution\n",
    "    ax = axes[1, 0]\n",
    "    cv_data = df_transient['spread_cv'].dropna()\n",
    "    cv_data_clean = cv_data[cv_data < 0.2]\n",
    "    if len(cv_data_clean) > 0:\n",
    "        ax.hist(cv_data_clean, bins=30, color='#f39c12', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=TRANSIENT_CONFIG['CV_THRESHOLD'], color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Threshold={TRANSIENT_CONFIG[\"CV_THRESHOLD\"]}')\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('Spread CV')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Spread CV Distribution')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 5. Half-life distribution\n",
    "    ax = axes[1, 1]\n",
    "    hl_data = df_transient['half_life'].replace([np.inf], np.nan).dropna()\n",
    "    hl_data_clean = hl_data[hl_data < 50]\n",
    "    if len(hl_data_clean) > 0:\n",
    "        ax.hist(hl_data_clean, bins=30, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=TRANSIENT_CONFIG['HL_MAX'], color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Max={TRANSIENT_CONFIG[\"HL_MAX\"]}h')\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('Half-Life (hours)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Half-Life Distribution')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 6. Hedge drift distribution\n",
    "    ax = axes[1, 2]\n",
    "    drift_data = df_transient['hedge_drift'].dropna()\n",
    "    drift_clean = drift_data[drift_data < 1.0]\n",
    "    if len(drift_clean) > 0:\n",
    "        ax.hist(drift_clean, bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=TRANSIENT_CONFIG['HEDGE_DRIFT_MAX'], color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Max={TRANSIENT_CONFIG[\"HEDGE_DRIFT_MAX\"]}')\n",
    "        ax.legend()\n",
    "    ax.set_xlabel('Hedge Ratio Drift')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Hedge Ratio Drift Distribution')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# INDIVIDUAL OPPORTUNITY VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "if len(df_transient) > 0 and df_transient['passed'].sum() > 0:\n",
    "    passed_events = df_transient[df_transient['passed']].sort_values('total_pnl', ascending=False)\n",
    "    selected = passed_events.head(6)\n",
    "\n",
    "    print(f\"Visualizing top {len(selected)} opportunities by P&L:\\n\")\n",
    "\n",
    "    for fig_num, (_, opp) in enumerate(selected.iterrows(), 1):\n",
    "        ticker_a = opp['ticker_a']\n",
    "        ticker_b = opp['ticker_b']\n",
    "        timestamp = opp['timestamp']\n",
    "        lag = TRANSIENT_CONFIG['EXECUTION_LAG']\n",
    "        cal_len = TRANSIENT_CONFIG['CALIBRATION']\n",
    "        exploit_len = TRANSIENT_CONFIG['EXPLOITATION']\n",
    "        total_need = lag + cal_len + exploit_len\n",
    "\n",
    "        prices_a = get_price_window(ticker_a, timestamp, total_need, ts_df, direction='forward')\n",
    "        prices_b = get_price_window(ticker_b, timestamp, total_need, ts_df, direction='forward')\n",
    "\n",
    "        if len(prices_a) < total_need or len(prices_b) < total_need:\n",
    "            continue\n",
    "\n",
    "        common_idx = prices_a.index.intersection(prices_b.index)\n",
    "        pa = prices_a.loc[common_idx]\n",
    "        pb = prices_b.loc[common_idx]\n",
    "\n",
    "        # Windows\n",
    "        pa_cal = pa.iloc[lag:lag + cal_len]\n",
    "        pb_cal = pb.iloc[lag:lag + cal_len]\n",
    "        pa_exp = pa.iloc[lag + cal_len:lag + cal_len + exploit_len]\n",
    "        pb_exp = pb.iloc[lag + cal_len:lag + cal_len + exploit_len]\n",
    "\n",
    "        beta_cal = opp['beta_cal']\n",
    "        spread_exp = pa_exp - beta_cal * pb_exp\n",
    "        spread_mean = spread_exp.mean()\n",
    "        spread_std = spread_exp.std()\n",
    "\n",
    "        lookback = min(20, len(spread_exp) // 2)\n",
    "        if lookback < 5:\n",
    "            lookback = 5\n",
    "        signals = zscore_signals(spread_exp, lookback=lookback, entry_z=2.0, exit_z=0.5)\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 8))\n",
    "        gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.35, wspace=0.3)\n",
    "\n",
    "        # Panel 1: Normalized prices with window boundaries\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        norm_a = (pa / pa.iloc[0] - 1) * 100\n",
    "        norm_b = (pb / pb.iloc[0] - 1) * 100\n",
    "        ax1.plot(pa.index, norm_a, label=ticker_a, color='#3498db', linewidth=2)\n",
    "        ax1.plot(pb.index, norm_b, label=ticker_b, color='#e74c3c', linewidth=2)\n",
    "        # Mark calibration / exploitation boundaries\n",
    "        if lag < len(pa):\n",
    "            ax1.axvline(x=pa.index[lag], color='gray', linestyle=':', linewidth=1.5, label='Cal start')\n",
    "        if lag + cal_len < len(pa):\n",
    "            ax1.axvline(x=pa.index[lag + cal_len], color='orange', linestyle='--', linewidth=1.5, label='Exploit start')\n",
    "        ax1.set_ylabel('% Change')\n",
    "        ax1.set_title(f'#{fig_num} {ticker_a}-{ticker_b} Prices')\n",
    "        ax1.legend(fontsize=8)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Panel 2: Spread with bands\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.plot(spread_exp.index, spread_exp, color='#2ecc71', linewidth=2, label='Spread')\n",
    "        ax2.axhline(y=spread_mean, color='blue', linewidth=1.5)\n",
    "        ax2.axhline(y=spread_mean + 2*spread_std, color='red', linestyle='--', linewidth=1.5)\n",
    "        ax2.axhline(y=spread_mean - 2*spread_std, color='red', linestyle='--', linewidth=1.5)\n",
    "        ax2.fill_between(spread_exp.index, spread_mean - 2*spread_std, spread_mean + 2*spread_std,\n",
    "                         alpha=0.1, color='red')\n",
    "        ax2.set_ylabel('Spread')\n",
    "        ax2.set_title(f'Exploitation Spread (beta={beta_cal:.3f})')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Panel 3: Z-score + signals\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        z = signals['z_score'].dropna()\n",
    "        ax3.plot(z.index, z, color='#3498db', linewidth=1.5)\n",
    "        ax3.axhline(y=2.0, color='red', linestyle='--', linewidth=1.5)\n",
    "        ax3.axhline(y=-2.0, color='red', linestyle='--', linewidth=1.5)\n",
    "        ax3.axhline(y=0.5, color='green', linestyle=':', linewidth=1)\n",
    "        ax3.axhline(y=-0.5, color='green', linestyle=':', linewidth=1)\n",
    "        # Mark entry signals\n",
    "        long_entries = signals[signals['long_entry']].index\n",
    "        short_entries = signals[signals['short_entry']].index\n",
    "        if len(long_entries) > 0:\n",
    "            ax3.scatter(long_entries, z.loc[long_entries], color='green', s=80, zorder=5, label='Long entry')\n",
    "        if len(short_entries) > 0:\n",
    "            ax3.scatter(short_entries, z.loc[short_entries], color='red', s=80, zorder=5, label='Short entry')\n",
    "        ax3.set_ylabel('Z-Score')\n",
    "        ax3.set_title('Z-Score Signals')\n",
    "        ax3.legend(fontsize=8)\n",
    "        ax3.grid(alpha=0.3)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Panel 4: Metrics summary\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        ax4.axis('off')\n",
    "        metrics_text = (\n",
    "            f\"Correlation:   {opp['correlation']:.3f}\\n\"\n",
    "            f\"Spread CV:     {opp['spread_cv']:.4f}\\n\"\n",
    "            f\"Half-Life:     {opp['half_life']:.1f}h\\n\"\n",
    "            f\"Hedge Drift:   {opp['hedge_drift']:.3f}\\n\"\n",
    "            f\"R-squared:     {opp['r_squared_cal']:.3f}\\n\"\n",
    "            f\"---\\n\"\n",
    "            f\"Trades:        {opp['n_trades']}\\n\"\n",
    "            f\"Total P&L:     {opp['total_pnl']:.4f}\\n\"\n",
    "            f\"Win Rate:      {opp['win_rate']:.1%}\\n\"\n",
    "            f\"Sharpe:        {opp['sharpe']:.2f}\\n\"\n",
    "            f\"Max Drawdown:  {opp['max_drawdown']:.4f}\\n\"\n",
    "        )\n",
    "        ax4.text(0.1, 0.9, metrics_text, transform=ax4.transAxes, fontsize=12,\n",
    "                 verticalalignment='top', fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "        ax4.set_title('Metrics')\n",
    "\n",
    "        fig.suptitle(f'OPPORTUNITY #{fig_num}: {ticker_a}-{ticker_b} @ {timestamp}',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No passed transient events to visualize.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Signal Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# REAL-TIME SIGNAL FUNCTIONS (FIX 4)\n",
    "# ============================================================================\n",
    "# These replace the GradientBoosting prediction model from the old\n",
    "# optics-prediction notebook.  They provide actionable interface definitions\n",
    "# for production use.\n",
    "\n",
    "def detect_new_formations(cluster_history_latest, previous_clusters):\n",
    "    \"\"\"\n",
    "    Compare latest clustering snapshot to previous snapshot.\n",
    "    Identify new co-clustering events (pair transitions from\n",
    "    not-co-clustered to co-clustered).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_history_latest : DataFrame\n",
    "        Latest snapshot with columns ['Ticker', 'Cluster_ID'].\n",
    "    previous_clusters : dict\n",
    "        Mapping ticker -> cluster_id from the previous timestamp.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple (ticker_a, ticker_b) representing new formations.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    current_clusters = {}\n",
    "    for _, row in cluster_history_latest.iterrows():\n",
    "        if row['Cluster_ID'] != -1:\n",
    "            current_clusters[row['Ticker']] = row['Cluster_ID']\n",
    "\n",
    "    # Find pairs co-clustering now\n",
    "    current_pairs = set()\n",
    "    for cid in set(current_clusters.values()):\n",
    "        members = sorted([t for t, c in current_clusters.items() if c == cid])\n",
    "        for a, b in itertools.combinations(members, 2):\n",
    "            current_pairs.add((a, b))\n",
    "\n",
    "    # Find pairs co-clustering previously\n",
    "    prev_pairs = set()\n",
    "    for cid in set(previous_clusters.values()):\n",
    "        members = sorted([t for t, c in previous_clusters.items() if c == cid])\n",
    "        for a, b in itertools.combinations(members, 2):\n",
    "            prev_pairs.add((a, b))\n",
    "\n",
    "    # New formations = currently co-clustered but not previously\n",
    "    new_formations = current_pairs - prev_pairs\n",
    "    return list(new_formations)\n",
    "\n",
    "\n",
    "def generate_transient_signals(ticker_a, ticker_b, ts_df, config):\n",
    "    \"\"\"\n",
    "    For a given pair, compute the current z-score and return a signal dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict with CALIBRATION, EXECUTION_LAG, ENTRY_Z, EXIT_Z.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with z_score, signal ('long', 'short', 'exit', 'wait'), spread_cv, half_life.\n",
    "    \"\"\"\n",
    "    from validation.pair_validation import (\n",
    "        compute_hedge_ratio, spread_cv_normalized, half_life as hl_func,\n",
    "        zscore_signals,\n",
    "    )\n",
    "\n",
    "    cal_len = config.get('CALIBRATION', 10)\n",
    "    ticker_data_a = ts_df.xs(ticker_a, level='Ticker').sort_index()['Price']\n",
    "    ticker_data_b = ts_df.xs(ticker_b, level='Ticker').sort_index()['Price']\n",
    "\n",
    "    if len(ticker_data_a) < cal_len + 10 or len(ticker_data_b) < cal_len + 10:\n",
    "        return None\n",
    "\n",
    "    common = ticker_data_a.index.intersection(ticker_data_b.index)\n",
    "    pa = ticker_data_a.loc[common]\n",
    "    pb = ticker_data_b.loc[common]\n",
    "\n",
    "    # Calibration on last cal_len observations\n",
    "    pa_cal = pa.iloc[-cal_len - 10:-10]\n",
    "    pb_cal = pb.iloc[-cal_len - 10:-10]\n",
    "    pa_recent = pa.iloc[-10:]\n",
    "    pb_recent = pb.iloc[-10:]\n",
    "\n",
    "    beta, _, _ = compute_hedge_ratio(pa_cal, pb_cal)\n",
    "    spread = pa_recent - beta * pb_recent\n",
    "    cv = spread_cv_normalized(spread, pa_recent, pb_recent)\n",
    "    hl = hl_func(spread)\n",
    "\n",
    "    mu = spread.mean()\n",
    "    sd = spread.std()\n",
    "    z = (spread.iloc[-1] - mu) / sd if sd > 0 else 0.0\n",
    "\n",
    "    entry_z = config.get('ENTRY_Z', 2.0)\n",
    "    exit_z = config.get('EXIT_Z', 0.5)\n",
    "\n",
    "    if z >= entry_z:\n",
    "        signal = 'short'\n",
    "    elif z <= -entry_z:\n",
    "        signal = 'long'\n",
    "    elif abs(z) <= exit_z:\n",
    "        signal = 'exit'\n",
    "    else:\n",
    "        signal = 'wait'\n",
    "\n",
    "    return {\n",
    "        'ticker_a': ticker_a,\n",
    "        'ticker_b': ticker_b,\n",
    "        'z_score': float(z),\n",
    "        'signal': signal,\n",
    "        'beta': float(beta),\n",
    "        'spread_cv': float(cv) if not np.isnan(cv) else None,\n",
    "        'half_life': float(hl) if not np.isnan(hl) and hl != np.inf else None,\n",
    "    }\n",
    "\n",
    "\n",
    "def track_stable_pairs(pair_list, ts_df, config):\n",
    "    \"\"\"\n",
    "    Daily cointegration check and Bollinger position for a list of stable pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pair_list : list of (ticker_a, ticker_b)\n",
    "    config : dict (not used currently, placeholder for thresholds)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts with pair, coint_pval, z_score, signal.\n",
    "    \"\"\"\n",
    "    from validation.pair_validation import compute_hedge_ratio\n",
    "    from statsmodels.tsa.stattools import coint\n",
    "\n",
    "    results = []\n",
    "    for ticker_a, ticker_b in pair_list:\n",
    "        try:\n",
    "            da = ts_df.xs(ticker_a, level='Ticker').sort_index()['Price'].resample('1D').last().dropna()\n",
    "            db = ts_df.xs(ticker_b, level='Ticker').sort_index()['Price'].resample('1D').last().dropna()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        common = da.index.intersection(db.index)\n",
    "        if len(common) < 50:\n",
    "            continue\n",
    "\n",
    "        da = da.loc[common]\n",
    "        db = db.loc[common]\n",
    "\n",
    "        try:\n",
    "            _, pval, _ = coint(da, db)\n",
    "        except Exception:\n",
    "            pval = 1.0\n",
    "\n",
    "        beta, _, _ = compute_hedge_ratio(da, db)\n",
    "        spread = da - beta * db\n",
    "        mu = spread.rolling(20).mean()\n",
    "        sd = spread.rolling(20).std()\n",
    "        z_latest = (spread.iloc[-1] - mu.iloc[-1]) / sd.iloc[-1] if sd.iloc[-1] > 0 else 0.0\n",
    "\n",
    "        if z_latest >= 2.0:\n",
    "            signal = 'short'\n",
    "        elif z_latest <= -2.0:\n",
    "            signal = 'long'\n",
    "        elif abs(z_latest) <= 0.5:\n",
    "            signal = 'exit'\n",
    "        else:\n",
    "            signal = 'hold'\n",
    "\n",
    "        results.append({\n",
    "            'pair': f'{ticker_a}-{ticker_b}',\n",
    "            'coint_pval': float(pval),\n",
    "            'z_score': float(z_latest),\n",
    "            'signal': signal,\n",
    "            'beta': float(beta),\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Signal functions defined:\")\n",
    "print(\"  - detect_new_formations(cluster_history_latest, previous_clusters)\")\n",
    "print(\"  - generate_transient_signals(ticker_a, ticker_b, ts_df, config)\")\n",
    "print(\"  - track_stable_pairs(pair_list, ts_df, config)\")\n",
    "print(\"\\nThese replace the GradientBoosting prediction model.\")\n",
    "print(\"Ready for production integration.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}